#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@ProjectName: homalos-datacenter
@FileName   : hybrid_storage.py
@Date       : 2025/10/27
@Author     : Lumosylva
@Email      : donnymoving@gmail.com
@Software   : PyCharm
@Description: æ··åˆå­˜å‚¨ - æ™ºèƒ½è·¯ç”±SQLiteï¼ˆçƒ­æ•°æ®ï¼‰å’ŒCSVï¼ˆå†·æ•°æ®ï¼Œå»¶è¿Ÿå‹ç¼©ï¼‰
"""
import pandas as pd
import threading
import time
from datetime import datetime, timedelta
from typing import Optional
from collections import deque

from src.core.event import Event, EventType
from src.core.event_bus import EventBus
from src.core.object import TickData
from src.core.sqlite_storage import SQLiteStorage
from src.core.storage import DataStorage
from src.utils.log import get_logger


class HybridStorage:
    """
    æ··åˆå­˜å‚¨ - æ™ºèƒ½è·¯ç”±SQLiteå’ŒCSVå½’æ¡£ï¼ˆå»¶è¿Ÿå‹ç¼©ï¼‰
    
    å­˜å‚¨ç­–ç•¥ï¼š
    1. çƒ­æ•°æ®ï¼ˆè¿‘7å¤©ï¼‰ï¼šå†™å…¥SQLite + CSVæœªå‹ç¼©å½’æ¡£ï¼ˆäº¤æ˜“æ—¶é—´é«˜æ€§èƒ½ï¼‰
    2. å†·æ•°æ®ï¼ˆ7å¤©å‰ï¼‰ï¼šCSVå½’æ¡£ï¼ˆéäº¤æ˜“æ—¶é—´æ‰¹é‡å‹ç¼©ä¸ºtar.gzï¼‰
    
    æŸ¥è¯¢ç­–ç•¥ï¼š
    1. æŸ¥è¯¢è¿‘7å¤©æ•°æ®ï¼šä»SQLiteæŸ¥è¯¢ï¼ˆå¿«é€Ÿï¼‰
    2. æŸ¥è¯¢å†å²æ•°æ®ï¼šä»CSVå½’æ¡£æŸ¥è¯¢ï¼ˆéœ€è¦æ—¶è‡ªåŠ¨è§£å‹ï¼‰
    3. è·¨è¶Šæ—¶é—´æ®µï¼šåˆå¹¶SQLiteå’ŒCSVç»“æœ
    
    å‹ç¼©ç­–ç•¥ï¼š
    - äº¤æ˜“æ—¶é—´ï¼šå†™å…¥æœªå‹ç¼©CSVï¼Œæ€§èƒ½æœ€ä¼˜
    - éäº¤æ˜“æ—¶é—´ï¼šæ•´ä¸ªäº¤æ˜“æ—¥æ–‡ä»¶å¤¹æ‰“åŒ…ä¸ºtar.gzï¼Œå‹ç¼©ç‡æ›´é«˜
    """
    
    def __init__(self,
                 event_bus: Optional[EventBus] = None,
                 sqlite_db_path: str = "data/db",
                 parquet_tick_path: str = "data/csv/ticks",
                 parquet_kline_path: str = "data/csv/klines",
                 retention_days: int = 7,
                 flush_interval: int = 60,
                 max_buffer_size: int = 10000,
                 buffer_warning_threshold: float = 0.7,  # è­¦å‘Šé˜ˆå€¼ï¼ˆ70%ï¼‰
                 buffer_flush_threshold: float = 0.85,  # æå‰åˆ·æ–°é˜ˆå€¼ï¼ˆ85%ï¼‰
                 trading_day_manager = None):
        """
        åˆå§‹åŒ–æ··åˆå­˜å‚¨
        
        Args:
            event_bus: äº‹ä»¶æ€»çº¿ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™è‡ªåŠ¨è®¢é˜… TICK äº‹ä»¶ï¼‰
            sqlite_db_path: SQLiteæ•°æ®åº“è·¯å¾„
            parquet_tick_path: Tick CSVæ–‡ä»¶è·¯å¾„
            parquet_kline_path: Kçº¿ CSVæ–‡ä»¶è·¯å¾„
            retention_days: SQLiteæ•°æ®ä¿ç•™å¤©æ•°
            flush_interval: å®šæ—¶åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤60ç§’ï¼ˆ1åˆ†é’Ÿï¼‰
            max_buffer_size: ç¼“å†²åŒºä¸Šé™ï¼Œé»˜è®¤10000æ¡ï¼ˆé˜²æ­¢å†…å­˜çˆ†ç‚¸ï¼‰
            buffer_warning_threshold: è­¦å‘Šé˜ˆå€¼ï¼ˆ0.7 = 70%ï¼‰
            buffer_flush_threshold: æå‰åˆ·æ–°é˜ˆå€¼ï¼ˆ0.85 = 85%ï¼‰
            trading_day_manager: äº¤æ˜“æ—¥ç®¡ç†å™¨
        """
        self.logger = get_logger(self.__class__.__name__)
        
        # SQLiteå­˜å‚¨å±‚ï¼ˆçƒ­æ•°æ®ï¼‰
        self.sqlite_storage = SQLiteStorage(
            db_path=sqlite_db_path,
            retention_days=retention_days,
            trading_day_manager=trading_day_manager
        )
        
        # CSVå­˜å‚¨å±‚ï¼ˆå†·æ•°æ®ï¼‰- åˆ†åˆ«ç®¡ç†Tickå’ŒKçº¿
        self.parquet_tick_storage = DataStorage(
            base_path=parquet_tick_path,
            trading_day_manager=trading_day_manager
        )
        self.parquet_kline_storage = DataStorage(
            base_path=parquet_kline_path,
            trading_day_manager=trading_day_manager
        )
        
        self.retention_days = retention_days
        self.flush_interval = flush_interval
        self.max_buffer_size = max_buffer_size
        self.buffer_warning_threshold = buffer_warning_threshold
        self.buffer_flush_threshold = buffer_flush_threshold
        
        # è®¡ç®—å®é™…é˜ˆå€¼ï¼ˆæ¡æ•°ï¼‰
        self._warning_size = int(max_buffer_size * buffer_warning_threshold)  # 7000æ¡
        self._flush_size = int(max_buffer_size * buffer_flush_threshold)      # 8500æ¡
        
        # ç¼“å†²åŒºé”ï¼ˆä¿æŠ¤å¹¶å‘è®¿é—®ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±ï¼‰
        self._buffer_lock = threading.Lock()
        
        # Tick æ•°æ®ç¼“å†²åŒº
        self.tick_buffer: deque[TickData] = deque()
        
        # ç»Ÿè®¡è®¡æ•°å™¨
        self._tick_recv_count = 0
        
        # å®šæ—¶åˆ·æ–°çº¿ç¨‹
        self._flush_thread: Optional[threading.Thread] = None
        self._stop_flush = threading.Event()
        
        # å¯åŠ¨å®šæ—¶åˆ·æ–°çº¿ç¨‹
        self._start_flush_thread()
        
        # å¦‚æœæä¾›äº†äº‹ä»¶æ€»çº¿ï¼Œè®¢é˜… TICK äº‹ä»¶
        if event_bus:
            event_bus.subscribe(EventType.TICK, self._on_tick)
            self.logger.info(
                f"å·²è®¢é˜… TICK äº‹ä»¶ï¼Œå®šæ—¶åˆ·æ–°: {flush_interval}ç§’ï¼Œ"
                f"ç¼“å†²åŒº: {max_buffer_size}æ¡ï¼ˆâš ï¸{self._warning_size} / ğŸŸ¡{self._flush_size} / ğŸ”´{max_buffer_size}ï¼‰"
            )
        
        self.logger.info(
            f"æ··åˆå­˜å‚¨åˆå§‹åŒ–å®Œæˆï¼ŒSQLiteä¿ç•™{retention_days}å¤©ï¼Œ"
            f"ä¸‰çº§é˜ˆå€¼ç­–ç•¥: âš ï¸è­¦å‘Š{int(buffer_warning_threshold*100)}% / "
            f"ğŸŸ¡æå‰åˆ·æ–°{int(buffer_flush_threshold*100)}% / ğŸ”´ç´§æ€¥åˆ·æ–°100%"
        )
    
    def _start_flush_thread(self) -> None:
        """å¯åŠ¨å®šæ—¶åˆ·æ–°çº¿ç¨‹"""
        self._stop_flush.clear()
        self._flush_thread = threading.Thread(
            target=self._flush_worker,
            name="HybridStorage-FlushWorker",
            daemon=True
        )
        self._flush_thread.start()
        self.logger.info(f"å®šæ—¶åˆ·æ–°çº¿ç¨‹å·²å¯åŠ¨ï¼Œé—´éš”: {self.flush_interval}ç§’")
    
    def _flush_worker(self) -> None:
        """å®šæ—¶åˆ·æ–°å·¥ä½œçº¿ç¨‹ï¼ˆå·²é€‚é…é”æœºåˆ¶ + å¿«é€Ÿé¢„æ£€æŸ¥ä¼˜åŒ–ï¼‰"""
        last_flush_time = time.time()
        
        while not self._stop_flush.is_set():
            try:
                # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾åˆ·æ–°æ—¶é—´
                current_time = time.time()
                elapsed = current_time - last_flush_time
                
                if elapsed >= self.flush_interval:
                    # å¿«é€Ÿé¢„æ£€æŸ¥ï¼ˆæ— é”ï¼‰ï¼šé¿å…åœ¨ç¼“å†²åŒºä¸ºç©ºæ—¶ä»ç„¶è·å–é”
                    # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªæ— é”çš„å¿«é€Ÿæ£€æŸ¥ï¼Œå¯èƒ½ä¸å®Œå…¨å‡†ç¡®ï¼Œä½†å¯ä»¥å‡å°‘é”ç«äº‰
                    if len(self.tick_buffer) > 0:
                        # æŒé”æ£€æŸ¥å¹¶åˆ·æ–°ï¼ˆäºŒæ¬¡ç¡®è®¤ï¼‰
                        with self._buffer_lock:
                            buffer_size = len(self.tick_buffer)
                            if buffer_size > 0:
                                self.logger.info(
                                    f"â° å®šæ—¶è§¦å‘åˆ·æ–°ï¼Œç¼“å†²åŒº: {buffer_size} æ¡Tick"
                                )
                                self._flush_tick_buffer_locked()
                    
                    # æ— è®ºæ˜¯å¦åˆ·æ–°ï¼Œéƒ½é‡ç½®å®šæ—¶å™¨ï¼ˆé¿å…ç´¯ç§¯å»¶è¿Ÿï¼‰
                    last_flush_time = current_time
                
                # çŸ­æš‚ä¼‘çœ ï¼ˆé¿å…CPUå ç”¨ï¼‰
                time.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                self.logger.error(f"å®šæ—¶åˆ·æ–°çº¿ç¨‹å¼‚å¸¸: {e}", exc_info=True)
                time.sleep(5)  # å¼‚å¸¸åç­‰å¾…5ç§’
    
    def stop(self) -> None:
        """åœæ­¢å®šæ—¶åˆ·æ–°çº¿ç¨‹ï¼Œåˆ·æ–°å‰©ä½™ç¼“å†²åŒºï¼ˆå·²é€‚é…é”æœºåˆ¶ï¼‰"""
        self.logger.info("æ­£åœ¨åœæ­¢ HybridStorage...")
        
        # åœæ­¢å®šæ—¶åˆ·æ–°çº¿ç¨‹
        self._stop_flush.set()
        if self._flush_thread and self._flush_thread.is_alive():
            self._flush_thread.join(timeout=5.0)
            self.logger.info("å®šæ—¶åˆ·æ–°çº¿ç¨‹å·²åœæ­¢")
        
        # åˆ·æ–°å‰©ä½™ç¼“å†²åŒºï¼ˆæŒé”æ£€æŸ¥ï¼‰
        with self._buffer_lock:
            buffer_size = len(self.tick_buffer)
            if buffer_size > 0:
                self.logger.warning(
                    f"ä¼˜é›…å…³é—­ï¼šåˆ·æ–°å‰©ä½™ {buffer_size} æ¡Tick..."
                )
                self._flush_tick_buffer_locked()
                self.logger.info("âœ“ ç¼“å†²åŒºå·²åˆ·æ–°")
        
        self.logger.info("HybridStorage å·²åœæ­¢")
    
    def _on_tick(self, event: Event) -> None:
        """
        å¤„ç† TICK äº‹ä»¶ï¼ˆä¸‰çº§é˜ˆå€¼ç­–ç•¥ + é”ä¿æŠ¤ï¼‰
        
        Args:
            event: TICK äº‹ä»¶
            
        è§¦å‘ç­–ç•¥ï¼ˆä¸‰çº§é˜ˆå€¼ï¼‰ï¼š
            1. ä¸»ç­–ç•¥ï¼šå®šæ—¶åˆ·æ–°ï¼ˆæ¯60ç§’ï¼‰ - æ­£å¸¸è´Ÿè½½
            2. æå‰åˆ·æ–°ï¼šç¼“å†²åŒºè¾¾åˆ°85%æ—¶æå‰åˆ·æ–° - ä¸­é«˜è´Ÿè½½
            3. ç´§æ€¥åˆ·æ–°ï¼šç¼“å†²åŒºè¾¾åˆ°100%æ—¶ç´§æ€¥åˆ·æ–° - æé«˜è´Ÿè½½ï¼ˆå®‰å…¨é˜€ï¼‰
        
        çº¿ç¨‹å®‰å…¨ï¼š
            - ä½¿ç”¨ _buffer_lock ä¿æŠ¤ç¼“å†²åŒºçš„å¹¶å‘è®¿é—®
            - å¤åˆ¶+æ¸…ç©ºæ“ä½œåœ¨é”ä¿æŠ¤ä¸‹åŸå­æ‰§è¡Œ
            - å®é™…ä¿å­˜åœ¨åå°çº¿ç¨‹æ‰§è¡Œï¼Œé¿å…é˜»å¡Tickæ¥æ”¶
        """
        try:
            # è§£æ Tick æ•°æ®
            payload = event.payload
            if not payload or "data" not in payload:
                self.logger.warning("æ”¶åˆ°ç©ºpayloadæˆ–ç¼ºå°‘dataå­—æ®µçš„TICKäº‹ä»¶")
                return
            
            tick: TickData = payload["data"]
            if not tick:
                self.logger.warning("TICKäº‹ä»¶ä¸­çš„dataä¸ºç©º")
                return
            
            # ===== ä¸´ç•ŒåŒºï¼šæ·»åŠ åˆ°ç¼“å†²åŒºå¹¶æ£€æŸ¥é˜ˆå€¼ =====
            with self._buffer_lock:
                self.tick_buffer.append(tick)
                buffer_size = len(self.tick_buffer)
                self._tick_recv_count += 1
                
                # ğŸ”´ ç´§æ€¥åˆ·æ–°ï¼ˆ100%ï¼‰ï¼šç¼“å†²åŒºå·²æ»¡ï¼ˆå®‰å…¨é˜€ï¼‰
                if buffer_size >= self.max_buffer_size:
                    buffer_usage = buffer_size / self.max_buffer_size * 100
                    self.logger.error(
                        f"ğŸ”´ ç¼“å†²åŒºå·²æ»¡ ({buffer_size}/{self.max_buffer_size} æ¡, {buffer_usage:.1f}%)ï¼Œ"
                        f"è§¦å‘ç´§æ€¥åˆ·æ–°ï¼ˆå®‰å…¨é˜€ï¼‰"
                    )
                    self._flush_tick_buffer_locked()
                    return
                
                # ğŸŸ¡ æå‰åˆ·æ–°ï¼ˆ85%ï¼‰ï¼šç¼“å†²åŒºæ¥è¿‘æ»¡ï¼ˆä¸»åŠ¨é˜²å¾¡ï¼‰
                if buffer_size >= self._flush_size:
                    buffer_usage = buffer_size / self.max_buffer_size * 100
                    self.logger.warning(
                        f"ğŸŸ¡ ç¼“å†²åŒºè¾¾åˆ°åˆ·æ–°é˜ˆå€¼ ({buffer_size}/{self.max_buffer_size} æ¡, {buffer_usage:.1f}%)ï¼Œ"
                        f"è§¦å‘æå‰åˆ·æ–°"
                    )
                    self._flush_tick_buffer_locked()
                    return
                
                # âš ï¸ è­¦å‘Šï¼ˆ70%ï¼‰ï¼šç¼“å†²åŒºä½¿ç”¨ç‡åé«˜ï¼ˆä»…è®°å½•æ—¥å¿—ï¼Œæ¯1000æ¡æ‰“å°ä¸€æ¬¡ï¼‰
                if buffer_size >= self._warning_size:
                    if self._tick_recv_count % 1000 == 0:
                        buffer_usage = buffer_size / self.max_buffer_size * 100
                        self.logger.warning(
                            f"âš ï¸ ç¼“å†²åŒºä½¿ç”¨ç‡åé«˜ ({buffer_size}/{self.max_buffer_size} æ¡, {buffer_usage:.1f}%)ï¼Œ"
                            f"ç­‰å¾…å®šæ—¶åˆ·æ–°æˆ–æå‰åˆ·æ–°"
                        )
                    return
            
            # ===== æ­£å¸¸æ—¥å¿—ï¼ˆåœ¨ä¸´ç•ŒåŒºå¤–ï¼Œé¿å…æŒé”æ—¶é—´è¿‡é•¿ï¼‰=====
            if self._tick_recv_count % 1000 == 0:
                # å¿«é€Ÿè·å–ç¼“å†²åŒºå¤§å°
                with self._buffer_lock:
                    buffer_size = len(self.tick_buffer)
                buffer_usage = buffer_size / self.max_buffer_size * 100
                self.logger.info(
                    f"âœ“ HybridStorageå·²æ¥æ”¶ {self._tick_recv_count} æ¡Tick | "
                    f"ç¼“å†²åŒº: {buffer_size}/{self.max_buffer_size} ({buffer_usage:.1f}%)"
                )
        
        except Exception as e:
            self.logger.error(f"å¤„ç† TICK äº‹ä»¶å¤±è´¥: {e}", exc_info=True)
    
    def _flush_tick_buffer_locked(self) -> None:
        """
        åˆ·æ–° Tick ç¼“å†²åŒºï¼ˆæŒé”ç‰ˆæœ¬ï¼Œè°ƒç”¨å‰å¿…é¡»æŒæœ‰_buffer_lockï¼‰
        
        å…³é”®è®¾è®¡ï¼š
        1. å¤åˆ¶å¹¶æ¸…ç©ºç¼“å†²åŒºï¼ˆåŸå­æ“ä½œï¼Œåœ¨é”ä¿æŠ¤ä¸‹ï¼‰
        2. ç«‹å³é‡Šæ”¾é”ï¼ˆé¿å…é˜»å¡Tickæ¥æ”¶ï¼‰
        3. åœ¨åå°çº¿ç¨‹æ‰§è¡Œå®é™…ä¿å­˜ï¼ˆè€—æ—¶æ“ä½œï¼‰
        
        Note:
            - æ­¤æ–¹æ³•å¿…é¡»åœ¨æŒæœ‰_buffer_lockçš„æƒ…å†µä¸‹è°ƒç”¨
            - è°ƒç”¨è€…è´Ÿè´£æŒæœ‰é”ï¼Œæœ¬æ–¹æ³•ä¸åŠ é”
            - æ¸…ç©ºåçš„ç¼“å†²åŒºå¯ç«‹å³æ¥æ”¶æ–°Tickï¼Œä¸ä¼šä¸¢å¤±æ•°æ®
        """
        if not self.tick_buffer:
            return
        
        # ===== ä¸´ç•ŒåŒºï¼šå¤åˆ¶å¹¶æ¸…ç©ºï¼ˆåŸå­æ“ä½œï¼‰=====
        ticks_to_save = list(self.tick_buffer)
        self.tick_buffer.clear()
        # æ³¨æ„ï¼šæ­¤æ—¶é”ä»ç”±è°ƒç”¨è€…æŒæœ‰ï¼Œåœ¨withè¯­å¥ç»“æŸæ—¶è‡ªåŠ¨é‡Šæ”¾
        # æ¸…ç©ºåï¼Œæ–°çš„Tickå¯ä»¥ç«‹å³è¿›å…¥ç©ºç¼“å†²åŒºï¼Œä¸ä¼šä¸¢å¤±
        
        self.logger.info(f"â†’ å‡†å¤‡åˆ·æ–° {len(ticks_to_save)} æ¡Tickåˆ°å­˜å‚¨å±‚...")
        
        # ===== åœ¨åå°çº¿ç¨‹æ‰§è¡Œä¿å­˜ï¼ˆé¿å…é˜»å¡Tickæ¥æ”¶ï¼‰=====
        threading.Thread(
            target=self._do_save_ticks,
            args=(ticks_to_save,),
            name=f"TickSaver-{len(ticks_to_save)}",
            daemon=True
        ).start()
    
    def _flush_tick_buffer(self) -> None:
        """åˆ·æ–° Tick ç¼“å†²åŒºåˆ°å­˜å‚¨å±‚ï¼ˆå…¼å®¹æ—§æ¥å£ï¼Œå†…éƒ¨åŠ é”ï¼‰"""
        with self._buffer_lock:
            self._flush_tick_buffer_locked()
    
    def _do_save_ticks(self, ticks_to_save: list[TickData]) -> None:
        """
        æ‰§è¡Œå®é™…çš„Tickæ•°æ®ä¿å­˜ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼‰
        
        Args:
            ticks_to_save: å¾…ä¿å­˜çš„Tickæ•°æ®åˆ—è¡¨
        """
        try:
            self.logger.info(f"â†’ å¼€å§‹ä¿å­˜ {len(ticks_to_save)} æ¡Tickåˆ°å­˜å‚¨å±‚...")
            
            # å°† TickData å¯¹è±¡è½¬æ¢ä¸º DataFrameï¼ˆ45ä¸ªå­—æ®µï¼ŒPascalCaseå‘½åï¼‰
            tick_dicts = []
            for tick in ticks_to_save:
                # æ„å»º Timestampï¼ˆå®Œæ•´datetimeç”¨äºæ—¶é—´åºåˆ—æŸ¥è¯¢ï¼‰
                timestamp_val = None
                if tick.trading_day and tick.update_time:
                    try:
                        timestamp_val = pd.to_datetime(f"{tick.trading_day} {tick.update_time}.{tick.update_millisec:03d}")
                    except Exception:
                        timestamp_val = pd.to_datetime(f"{tick.trading_day} {tick.update_time}")
                
                # æŒ‰ç”¨æˆ·æŒ‡å®šçš„45ä¸ªå­—æ®µé¡ºåºï¼ˆPascalCaseå‘½åï¼‰
                tick_dict = {
                    # 1-2: åŸºç¡€æ—¶é—´ä¿¡æ¯
                    "TradingDay": tick.trading_day,
                    "ExchangeID": tick.exchange_id.value if tick.exchange_id else None,
                    
                    # 3-5: ä»·æ ¼ä¿¡æ¯
                    "LastPrice": tick.last_price,
                    "PreSettlementPrice": tick.pre_settlement_price,
                    "PreClosePrice": tick.pre_close_price,
                    
                    # 6-10: æˆäº¤æŒä»“
                    "PreOpenInterest": tick.pre_open_interest,
                    "OpenPrice": tick.open_price,
                    "HighestPrice": tick.highest_price,
                    "LowestPrice": tick.lowest_price,
                    "Volume": tick.volume,
                    
                    # 11-14: ç»Ÿè®¡æ•°æ®
                    "Turnover": tick.turnover,
                    "OpenInterest": tick.open_interest,
                    "ClosePrice": tick.close_price,
                    "SettlementPrice": tick.settlement_price,
                    
                    # 15-18: æ¶¨è·Œåœå’ŒDelta
                    "UpperLimitPrice": tick.upper_limit_price,
                    "LowerLimitPrice": tick.lower_limit_price,
                    "PreDelta": tick.pre_delta,
                    "CurrDelta": tick.curr_delta,
                    
                    # 19-20: æ›´æ–°æ—¶é—´
                    "UpdateTime": tick.update_time,
                    "UpdateMillisec": tick.update_millisec,
                    
                    # 21-24: ä¹°ä¸€æ¡£
                    "BidPrice1": tick.bid_price_1,
                    "BidVolume1": tick.bid_volume_1,
                    "AskPrice1": tick.ask_price_1,
                    "AskVolume1": tick.ask_volume_1,
                    
                    # 25-28: ä¹°å–äºŒæ¡£
                    "BidPrice2": tick.bid_price_2,
                    "BidVolume2": tick.bid_volume_2,
                    "AskPrice2": tick.ask_price_2,
                    "AskVolume2": tick.ask_volume_2,
                    
                    # 29-32: ä¹°å–ä¸‰æ¡£
                    "BidPrice3": tick.bid_price_3,
                    "BidVolume3": tick.bid_volume_3,
                    "AskPrice3": tick.ask_price_3,
                    "AskVolume3": tick.ask_volume_3,
                    
                    # 33-36: ä¹°å–å››æ¡£
                    "BidPrice4": tick.bid_price_4,
                    "BidVolume4": tick.bid_volume_4,
                    "AskPrice4": tick.ask_price_4,
                    "AskVolume4": tick.ask_volume_4,
                    
                    # 37-40: ä¹°å–äº”æ¡£
                    "BidPrice5": tick.bid_price_5,
                    "BidVolume5": tick.bid_volume_5,
                    "AskPrice5": tick.ask_price_5,
                    "AskVolume5": tick.ask_volume_5,
                    
                    # 41-45: å…¶ä»–ä¿¡æ¯å’Œæ—¶é—´æˆ³
                    "AveragePrice": tick.average_price,
                    "ActionDay": tick.action_day,
                    "InstrumentID": tick.instrument_id,
                    "ExchangeInstID": tick.exchange_inst_id,
                    "BandingUpperPrice": tick.banding_upper_price,
                    "BandingLowerPrice": tick.banding_lower_price,
                    "Timestamp": timestamp_val,
                }
                tick_dicts.append(tick_dict)
            
            df = pd.DataFrame(tick_dicts)
            
            # æ‰¹é‡ä¿å­˜
            self.save_ticks(df)
            
            self.logger.info(f"âœ“ å·²æ‰¹é‡ä¿å­˜ {len(tick_dicts)} æ¡ Tick æ•°æ®åˆ°å­˜å‚¨å±‚ï¼ˆåŒ…å«å®Œæ•´å­—æ®µï¼‰")
        
        except Exception as e:
            self.logger.error(f"åˆ·æ–° Tick ç¼“å†²åŒºå¤±è´¥: {e}", exc_info=True)
    
    def save_ticks(self, df: pd.DataFrame) -> None:
        """
        ä¿å­˜Tickæ•°æ®ï¼ˆåŒæ—¶ä¿å­˜åˆ°SQLiteå’ŒParquetï¼‰
        
        Args:
            df: Tickæ•°æ®DataFrame
        """
        if df.empty:
            return
        
        try:
            self.logger.info(f"  â†’ ä¿å­˜ {len(df)} æ¡Tickåˆ°SQLite...")
            # 1. ä¿å­˜åˆ°SQLiteï¼ˆçƒ­æ•°æ®ï¼Œå¿«é€ŸæŸ¥è¯¢ï¼‰
            self.sqlite_storage.save_ticks(df)
            self.logger.info("  âœ“ SQLiteä¿å­˜æˆåŠŸ")
            
            # 2. ä¿å­˜åˆ°CSVå½’æ¡£ï¼ˆå†·æ•°æ®ï¼ŒæŒ‰äº¤æ˜“æ—¥ç»Ÿä¸€ä¿å­˜ï¼‰
            # ä¸ä¼ dateå‚æ•°ï¼Œä½¿ç”¨trading_day_managerçš„äº¤æ˜“æ—¥
            if "instrument_id" in df.columns:
                parquet_count = 0
                for instrument_id, group in df.groupby("instrument_id"):
                    self.parquet_tick_storage.save_ticks(
                        symbol=str(instrument_id),
                        df=group,
                        date=None  # ä½¿ç”¨trading_day_managerçš„äº¤æ˜“æ—¥
                    )
                    parquet_count += len(group)
                self.logger.info(f"  âœ“ CSVä¿å­˜æˆåŠŸ ({parquet_count}æ¡ï¼Œæœªå‹ç¼©ï¼Œäº¤æ˜“æ—¥ç»Ÿä¸€)")
        
        except Exception as e:
            self.logger.error(f"ä¿å­˜Tickæ•°æ®å¤±è´¥: {e}", exc_info=True)
    
    def save_klines(self, df: pd.DataFrame) -> None:
        """
        ä¿å­˜Kçº¿æ•°æ®ï¼ˆåŒæ—¶ä¿å­˜åˆ°SQLiteå’ŒCSVå½’æ¡£ï¼‰
        
        Args:
            df: Kçº¿æ•°æ®DataFrameï¼ˆåŒ…å«13ä¸ªæ ¸å¿ƒå­—æ®µï¼ŒPascalCaseå‘½åï¼‰
        """
        if df.empty:
            return
        
        try:
            # éªŒè¯å¿…è¦å­—æ®µï¼ˆPascalCaseå‘½åï¼‰
            required_cols = ["InstrumentID", "BarType", "Timestamp"]
            if not all(col in df.columns for col in required_cols):
                self.logger.warning(f"Kçº¿æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {required_cols}ï¼Œå®é™…å­—æ®µ: {df.columns.tolist()}")
                return
            
            # 1. ä¿å­˜åˆ°SQLiteï¼ˆçƒ­æ•°æ®ï¼Œå¿«é€ŸæŸ¥è¯¢ï¼‰
            self.sqlite_storage.save_klines(df)
            
            # 2. ä¿å­˜åˆ°CSVå½’æ¡£ï¼ˆå†·æ•°æ®ï¼ŒæŒ‰äº¤æ˜“æ—¥ç»Ÿä¸€ä¿å­˜ï¼‰
            # ä¸ä¼ dateå‚æ•°ï¼Œä½¿ç”¨trading_day_managerçš„äº¤æ˜“æ—¥
            for (instrument_id, bar_type), group in df.groupby(["InstrumentID", "BarType"]):
                symbol_with_interval = f"{instrument_id}_{bar_type}"
                self.parquet_kline_storage.save_kline(
                    symbol=symbol_with_interval,
                    df=group,
                    date=None  # ä½¿ç”¨trading_day_managerçš„äº¤æ˜“æ—¥
                )
        
        except Exception as e:
            self.logger.error(f"ä¿å­˜Kçº¿æ•°æ®å¤±è´¥: {e}", exc_info=True)
    
    def query_ticks(self,
                    instrument_id: str,
                    start_time: str,
                    end_time: str) -> pd.DataFrame:
        """
        æŸ¥è¯¢Tickæ•°æ®ï¼ˆæ™ºèƒ½è·¯ç”±ï¼‰
        
        Args:
            instrument_id: åˆçº¦ä»£ç 
            start_time: å¼€å§‹æ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
        
        Returns:
            Tickæ•°æ®DataFrame
        """
        try:
            start_dt = pd.to_datetime(start_time)
            end_dt = pd.to_datetime(end_time)
            cutoff_dt = datetime.now() - timedelta(days=self.retention_days)
            
            results = []
            
            # 1. æŸ¥è¯¢å†å²æ•°æ®ï¼ˆParquetï¼‰
            if start_dt < cutoff_dt:
                parquet_end = min(end_dt, cutoff_dt)
                df_parquet = self._query_ticks_from_parquet(
                    instrument_id,
                    start_dt,
                    parquet_end
                )
                if not df_parquet.empty:
                    results.append(df_parquet)
                
                self.logger.debug(
                    f"ä»CSVå½’æ¡£æŸ¥è¯¢åˆ° {len(df_parquet)} æ¡å†å²Tickæ•°æ®"
                )
            
            # 2. æŸ¥è¯¢è¿‘æœŸæ•°æ®ï¼ˆSQLiteï¼‰
            if end_dt >= cutoff_dt:
                sqlite_start = max(start_dt, cutoff_dt)
                df_sqlite = self.sqlite_storage.query_ticks(
                    instrument_id,
                    sqlite_start.isoformat(),
                    end_dt.isoformat()
                )
                if not df_sqlite.empty:
                    results.append(df_sqlite)
                
                self.logger.debug(
                    f"ä»SQLiteæŸ¥è¯¢åˆ° {len(df_sqlite)} æ¡è¿‘æœŸTickæ•°æ®"
                )
            
            # 3. åˆå¹¶ç»“æœ
            if results:
                df = pd.concat(results, ignore_index=True)
                df = df.sort_values("datetime")
                return df
            
            return pd.DataFrame()
        
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢Tickæ•°æ®å¤±è´¥: {e}", exc_info=True)
            return pd.DataFrame()
    
    def query_klines(self,
                     instrument_id: str,
                     interval: str,
                     start_time: str,
                     end_time: str) -> pd.DataFrame:
        """
        æŸ¥è¯¢Kçº¿æ•°æ®ï¼ˆæ™ºèƒ½è·¯ç”±ï¼‰
        
        Args:
            instrument_id: åˆçº¦ä»£ç 
            interval: Kçº¿å‘¨æœŸ
            start_time: å¼€å§‹æ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
        
        Returns:
            Kçº¿æ•°æ®DataFrame
        """
        try:
            start_dt = pd.to_datetime(start_time)
            end_dt = pd.to_datetime(end_time)
            cutoff_dt = datetime.now() - timedelta(days=self.retention_days)
            
            results = []
            
            # 1. æŸ¥è¯¢å†å²æ•°æ®ï¼ˆParquetï¼‰
            if start_dt < cutoff_dt:
                parquet_end = min(end_dt, cutoff_dt)
                df_parquet = self._query_klines_from_parquet(
                    instrument_id,
                    interval,
                    start_dt,
                    parquet_end
                )
                if not df_parquet.empty:
                    results.append(df_parquet)
                
                self.logger.debug(
                    f"ä»CSVå½’æ¡£æŸ¥è¯¢åˆ° {len(df_parquet)} æ¡å†å²Kçº¿æ•°æ®"
                )
            
            # 2. æŸ¥è¯¢è¿‘æœŸæ•°æ®ï¼ˆSQLiteï¼‰
            if end_dt >= cutoff_dt:
                sqlite_start = max(start_dt, cutoff_dt)
                df_sqlite = self.sqlite_storage.query_klines(
                    instrument_id,
                    interval,
                    sqlite_start.isoformat(),
                    end_dt.isoformat()
                )
                if not df_sqlite.empty:
                    results.append(df_sqlite)
                
                self.logger.debug(
                    f"ä»SQLiteæŸ¥è¯¢åˆ° {len(df_sqlite)} æ¡è¿‘æœŸKçº¿æ•°æ®"
                )
            
            # 3. åˆå¹¶ç»“æœ
            if results:
                df = pd.concat(results, ignore_index=True)
                df = df.sort_values("datetime")
                return df
            
            return pd.DataFrame()
        
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢Kçº¿æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return pd.DataFrame()
    
    def _query_ticks_from_parquet(self,
                                   instrument_id: str,
                                   start_dt: datetime,
                                   end_dt: datetime) -> pd.DataFrame:
        """
        ä»ParquetæŸ¥è¯¢Tickæ•°æ®
        
        Args:
            instrument_id: åˆçº¦ä»£ç 
            start_dt: å¼€å§‹æ—¶é—´
            end_dt: ç»“æŸæ—¶é—´
        
        Returns:
            Tickæ•°æ®DataFrame
        """
        results = []
        
        # éå†æ—¥æœŸèŒƒå›´
        current_date = start_dt.date()
        end_date = end_dt.date()
        
        while current_date <= end_date:
            # è®¡ç®—å½“å¤©çš„å¼€å§‹å’Œç»“æŸæ—¶é—´
            day_start = datetime.combine(current_date, datetime.min.time())
            day_end = datetime.combine(current_date, datetime.max.time())
            
            # ä½¿ç”¨ start_time å’Œ end_time å‚æ•°
            df = self.parquet_tick_storage.query_ticks(
                symbol=instrument_id,
                start_time=max(day_start, start_dt).isoformat(),
                end_time=min(day_end, end_dt).isoformat()
            )
            if not df.empty:
                results.append(df)
            
            current_date += timedelta(days=1)
        
        if results:
            return pd.concat(results, ignore_index=True)
        
        return pd.DataFrame()
    
    def _query_klines_from_parquet(self,
                                    instrument_id: str,
                                    interval: str,
                                    start_dt: datetime,
                                    end_dt: datetime) -> pd.DataFrame:
        """
        ä»ParquetæŸ¥è¯¢Kçº¿æ•°æ®
        
        Args:
            instrument_id: åˆçº¦ä»£ç 
            interval: Kçº¿å‘¨æœŸ
            start_dt: å¼€å§‹æ—¶é—´
            end_dt: ç»“æŸæ—¶é—´
        
        Returns:
            Kçº¿æ•°æ®DataFrame
        """
        results = []
        
        # éå†æ—¥æœŸèŒƒå›´
        current_date = start_dt.date()
        end_date = end_dt.date()
        symbol_with_interval = f"{instrument_id}_{interval}"
        
        while current_date <= end_date:
            # è®¡ç®—å½“å¤©çš„å¼€å§‹å’Œç»“æŸæ—¶é—´
            day_start = datetime.combine(current_date, datetime.min.time())
            day_end = datetime.combine(current_date, datetime.max.time())
            
            # ä½¿ç”¨ start_time å’Œ end_time å‚æ•°
            df = self.parquet_kline_storage.query_kline(
                symbol=symbol_with_interval,
                start_time=max(day_start, start_dt).isoformat(),
                end_time=min(day_end, end_dt).isoformat()
            )
            if not df.empty:
                results.append(df)
            
            current_date += timedelta(days=1)
        
        if results:
            return pd.concat(results, ignore_index=True)
        
        return pd.DataFrame()
    
    def get_statistics(self) -> dict:
        """
        è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        sqlite_stats = self.sqlite_storage.get_statistics()
        # parquet_stats = self.parquet_storage.get_statistics()  # å¦‚æœæœ‰çš„è¯
        
        return {
            "storage_type": "hybrid (SQLite + CSV, å»¶è¿Ÿå‹ç¼©)",
            "retention_days": self.retention_days,
            "sqlite": sqlite_stats,
            # "parquet": parquet_stats
        }

