#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@ProjectName: homalos-datacenter
@FileName   : sqlite_storage.py
@Date       : 2025/10/27
@Author     : Lumosylva
@Email      : donnymoving@gmail.com
@Software   : PyCharm
@Description: SQLiteå­˜å‚¨å±‚ - ç”¨äºè¿‘æœŸæ•°æ®çš„å¿«é€ŸæŸ¥è¯¢
"""
import sqlite3
import pandas as pd  # type: ignore
import threading
import queue
import time
from pathlib import Path
from contextlib import contextmanager

from src.utils.log import get_logger


class SQLiteStorage:
    """
    SQLiteå­˜å‚¨å±‚ - ç”¨äºè¿‘æœŸæ•°æ®çš„å¿«é€ŸæŸ¥è¯¢
    
    ç‰¹ç‚¹ï¼š
    1. é«˜é€Ÿè¯»å†™ï¼šWALæ¨¡å¼ï¼Œæ”¯æŒå¹¶å‘
    2. ç´¢å¼•ä¼˜åŒ–ï¼šåˆçº¦ä»£ç +æ—¶é—´çš„å¤åˆç´¢å¼•
    3. è‡ªåŠ¨æ¸…ç†ï¼šå®šæœŸåˆ é™¤å·²å½’æ¡£æ•°æ®
    4. è½»é‡çº§ï¼šå•æ–‡ä»¶å­˜å‚¨ï¼Œæ˜“äºå¤‡ä»½
    """
    
    def __init__(self, 
                 db_path: str = "data/db",
                 retention_days: int = 7,
                 trading_day_manager = None):
        """
        åˆå§‹åŒ–SQLiteå­˜å‚¨å±‚ï¼ˆæŒ‰äº¤æ˜“æ—¥+åˆçº¦åˆ†åº“å­˜å‚¨ï¼‰
        
        Args:
            db_path: æ•°æ®åº“æ–‡ä»¶æ ¹ç›®å½•
            retention_days: æ•°æ®ä¿ç•™å¤©æ•°ï¼ˆé»˜è®¤7å¤©ï¼‰
            trading_day_manager: äº¤æ˜“æ—¥ç®¡ç†å™¨
            
        æ•°æ®åº“æ–‡ä»¶ç»“æ„ï¼š
            - data/db/tick/20251027/SA601.db
            - data/db/kline/20251027/SA601.db
        """
        self.db_root = Path(db_path)
        self.db_root.mkdir(parents=True, exist_ok=True)
        
        self.tick_db_root = self.db_root / "tick"
        self.kline_db_root = self.db_root / "kline"
        self.tick_db_root.mkdir(parents=True, exist_ok=True)
        self.kline_db_root.mkdir(parents=True, exist_ok=True)
        
        self.logger = get_logger(self.__class__.__name__)
        self.retention_days = retention_days
        self.trading_day_manager = trading_day_manager
        
        # å‘åå…¼å®¹å±æ€§ï¼ˆæŸ¥è¯¢æ–¹æ³•æš‚æœªé‡æ„ä¸ºåˆ†åº“æŸ¥è¯¢ï¼‰
        # TODO: é‡æ„æŸ¥è¯¢æ–¹æ³•ä»¥æ”¯æŒåˆ†åº“æŸ¥è¯¢
        self.tick_db_file = self.tick_db_root / "deprecated_single_file.db"
        self.kline_db_file = self.kline_db_root / "deprecated_single_file.db"
        
        # å†™å…¥é˜Ÿåˆ—ï¼ˆè§£å†³å¹¶å‘é”é—®é¢˜ï¼Œæ”¯æŒåŠ¨æ€æ‰©å®¹ï¼‰
        self._write_queue: "queue.Queue[tuple]" = queue.Queue(maxsize=0)  # æ— é™å¤§å°ï¼ŒåŠ¨æ€æ‰©å®¹
        self._write_thread: threading.Thread | None = None
        self._stop_event = threading.Event()
        
        # é˜Ÿåˆ—ç›‘æ§é˜ˆå€¼
        self._queue_warn_threshold = 5000   # è½¯æ€§å‘Šè­¦é˜ˆå€¼
        self._queue_critical_threshold = 20000  # ä¸¥é‡å‘Šè­¦é˜ˆå€¼
        self._last_queue_warn_time = 0.0  # ä¸Šæ¬¡å‘Šè­¦æ—¶é—´ï¼ˆé¿å…æ—¥å¿—åˆ·å±ï¼‰
        
        # å¯åŠ¨å†™å…¥çº¿ç¨‹
        self._start_write_thread()
        
        self.logger.info(
            f"SQLiteå­˜å‚¨å±‚åˆå§‹åŒ–å®Œæˆï¼ˆæŒ‰äº¤æ˜“æ—¥+åˆçº¦åˆ†åº“ï¼ŒåŠ¨æ€æ‰©å®¹é˜Ÿåˆ—ï¼‰ï¼Œ"
            f"æ•°æ®ä¿ç•™{retention_days}å¤©ï¼Œå·²å¯åŠ¨å•çº¿ç¨‹å†™å…¥é˜Ÿåˆ—"
        )
    
    def _get_db_path(self, data_type: str, instrument_id: str, trading_day: str) -> Path:
        """
        è·å–æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼ˆæŒ‰äº¤æ˜“æ—¥+åˆçº¦åˆ†åº“ï¼‰
        
        Args:
            data_type: æ•°æ®ç±»å‹ï¼ˆ'tick' æˆ– 'kline'ï¼‰
            instrument_id: åˆçº¦ä»£ç 
            trading_day: äº¤æ˜“æ—¥ï¼ˆYYYYMMDDï¼‰
            
        Returns:
            æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼ˆä¾‹å¦‚ï¼šdata/db/tick/20251027/SA601.dbï¼‰
        """
        if data_type == "tick":
            root = self.tick_db_root
        elif data_type == "kline":
            root = self.kline_db_root
        else:
            raise ValueError(f"æœªçŸ¥çš„æ•°æ®ç±»å‹: {data_type}")
        
        # data/db/tick/20251027/
        day_dir = root / trading_day
        day_dir.mkdir(parents=True, exist_ok=True)
        
        # data/db/tick/20251027/SA601.db
        return day_dir / f"{instrument_id}.db"
    
    def _init_tick_table(self, conn) -> None:
        """åˆå§‹åŒ–Tickè¡¨ç»“æ„"""
        with conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS ticks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    instrument_id TEXT NOT NULL,
                    exchange_id TEXT NOT NULL,
                    exchange_inst_id TEXT,
                    datetime TIMESTAMP NOT NULL,
                    trading_day TEXT NOT NULL,
                    action_day TEXT,
                    timestamp TIMESTAMP,
                    update_time TEXT,
                    update_millisec INTEGER,
                    last_price REAL,
                    open_price REAL,
                    highest_price REAL,
                    lowest_price REAL,
                    close_price REAL,
                    average_price REAL,
                    pre_settlement_price REAL,
                    pre_close_price REAL,
                    pre_open_interest REAL,
                    settlement_price REAL,
                    upper_limit_price REAL,
                    lower_limit_price REAL,
                    banding_upper_price REAL,
                    banding_lower_price REAL,
                    volume INTEGER,
                    turnover REAL,
                    open_interest REAL,
                    pre_delta REAL,
                    curr_delta REAL,
                    bid_price_1 REAL,
                    bid_volume_1 INTEGER,
                    ask_price_1 REAL,
                    ask_volume_1 INTEGER,
                    bid_price_2 REAL,
                    bid_volume_2 INTEGER,
                    ask_price_2 REAL,
                    ask_volume_2 INTEGER,
                    bid_price_3 REAL,
                    bid_volume_3 INTEGER,
                    ask_price_3 REAL,
                    ask_volume_3 INTEGER,
                    bid_price_4 REAL,
                    bid_volume_4 INTEGER,
                    ask_price_4 REAL,
                    ask_volume_4 INTEGER,
                    bid_price_5 REAL,
                    bid_volume_5 INTEGER,
                    ask_price_5 REAL,
                    ask_volume_5 INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_tick_symbol_time
                ON ticks(instrument_id, datetime)
            """)
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_tick_trading_day
                ON ticks(trading_day)
            """)
            
            # å¯ç”¨WALæ¨¡å¼ï¼ˆæé«˜å¹¶å‘æ€§èƒ½ï¼‰
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=10000")
    
    def _init_kline_table(self, conn) -> None:
        """åˆå§‹åŒ–Kçº¿è¡¨ç»“æ„"""
        with conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS klines (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    instrument_id TEXT NOT NULL,
                    exchange_id TEXT NOT NULL,
                    interval TEXT NOT NULL,
                    bar_type TEXT,
                    datetime TIMESTAMP NOT NULL,
                    timestamp TIMESTAMP,
                    trading_day TEXT,
                    update_time TEXT,
                    open REAL,
                    open_price REAL,
                    high REAL,
                    high_price REAL,
                    low REAL,
                    low_price REAL,
                    close REAL,
                    close_price REAL,
                    volume INTEGER,
                    last_volume INTEGER,
                    turnover REAL,
                    open_interest REAL,
                    last_open_interest REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # åˆ›å»ºç´¢å¼•
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_kline_symbol_interval_time
                ON klines(instrument_id, interval, datetime)
            """)
            
            # å¯ç”¨WALæ¨¡å¼
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=10000")
    
    def _check_queue_health(self) -> None:
        """
        æ£€æŸ¥é˜Ÿåˆ—å¥åº·çŠ¶æ€å¹¶è§¦å‘å‘Šè­¦
        
        é‡‡ç”¨åˆ†çº§å‘Šè­¦æœºåˆ¶ï¼š
        - 5000æ¡ï¼šè½¯æ€§å‘Šè­¦ï¼ˆ1åˆ†é’Ÿæœ€å¤š1æ¬¡ï¼‰
        - 20000æ¡ï¼šä¸¥é‡å‘Šè­¦ï¼ˆç«‹å³å‘Šè­¦ï¼‰
        """
        queue_size = self._write_queue.qsize()
        current_time = time.time()
        
        # ä¸¥é‡å‘Šè­¦ï¼ˆè¶…è¿‡20000æ¡ï¼‰
        if queue_size >= self._queue_critical_threshold:
            self.logger.error(
                f"ğŸ”´ CRITICAL: SQLiteå†™å…¥é˜Ÿåˆ—ä¸¥é‡ç§¯å‹ï¼"
                f"å½“å‰: {queue_size}æ¡ï¼ˆè¶…è¿‡ä¸¥é‡é˜ˆå€¼{self._queue_critical_threshold}ï¼‰ï¼Œ"
                f"å¯èƒ½å­˜åœ¨æ€§èƒ½ç“¶é¢ˆï¼Œè¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€ï¼"
            )
            self._last_queue_warn_time = current_time
            
        # è½¯æ€§å‘Šè­¦ï¼ˆè¶…è¿‡5000æ¡ï¼Œä¸”è·ç¦»ä¸Šæ¬¡å‘Šè­¦è¶…è¿‡60ç§’ï¼‰
        elif queue_size >= self._queue_warn_threshold:
            if current_time - self._last_queue_warn_time > 60:  # 60ç§’å†…åªå‘Šè­¦ä¸€æ¬¡
                self.logger.warning(
                    f"âš ï¸  SQLiteå†™å…¥é˜Ÿåˆ—ç§¯å‹: {queue_size}æ¡ "
                    f"(å‘Šè­¦é˜ˆå€¼: {self._queue_warn_threshold})"
                )
                self._last_queue_warn_time = current_time
    
    def _start_write_thread(self) -> None:
        """å¯åŠ¨å•ç‹¬çš„å†™å…¥çº¿ç¨‹ï¼ˆè§£å†³å¹¶å‘é”é—®é¢˜ï¼‰"""
        if self._write_thread is None or not self._write_thread.is_alive():
            self._stop_event.clear()
            self._write_thread = threading.Thread(
                target=self._write_worker,
                name="SQLiteWriteThread",
                daemon=True
            )
            self._write_thread.start()
            self.logger.info("SQLiteå†™å…¥çº¿ç¨‹å·²å¯åŠ¨")
    
    def _write_worker(self) -> None:
        """å†™å…¥çº¿ç¨‹å·¥ä½œå‡½æ•° - ä»é˜Ÿåˆ—ä¸­å–ä»»åŠ¡å¹¶ä¸²è¡Œå†™å…¥"""
        self.logger.info("SQLiteå†™å…¥çº¿ç¨‹å¼€å§‹å·¥ä½œ...")
        
        while not self._stop_event.is_set():
            try:
                # ä»é˜Ÿåˆ—è·å–å†™å…¥ä»»åŠ¡ï¼ˆè®¾ç½®è¶…æ—¶ä»¥ä¾¿æ£€æŸ¥stop_eventï¼‰
                task = self._write_queue.get(timeout=1.0)
                
                if task[0] == "stop":  # åœæ­¢ä¿¡å·
                    break
                
                # æ‰§è¡Œå†™å…¥ä»»åŠ¡
                task_type, args = task
                try:
                    if task_type == "tick":
                        self._do_write_ticks(args)
                    elif task_type == "kline":
                        self._do_write_klines(args)
                except Exception as e:
                    self.logger.error(f"å†™å…¥ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
                finally:
                    self._write_queue.task_done()
                    
            except queue.Empty:
                continue
            except Exception as e:
                self.logger.error(f"å†™å…¥çº¿ç¨‹å¼‚å¸¸: {e}", exc_info=True)
        
        self.logger.info("SQLiteå†™å…¥çº¿ç¨‹å·²åœæ­¢")
    
    def get_queue_stats(self) -> dict:
        """
        è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«é˜Ÿåˆ—çŠ¶æ€çš„å­—å…¸
        """
        queue_size = self._write_queue.qsize()
        return {
            "queue_size": queue_size,
            "warn_threshold": self._queue_warn_threshold,
            "critical_threshold": self._queue_critical_threshold,
            "health_status": (
                "critical" if queue_size >= self._queue_critical_threshold
                else "warning" if queue_size >= self._queue_warn_threshold
                else "healthy"
            ),
            "usage_percent": round(queue_size / self._queue_warn_threshold * 100, 2) if queue_size > 0 else 0
        }
    
    def stop(self) -> None:
        """åœæ­¢å†™å…¥çº¿ç¨‹"""
        if self._write_thread and self._write_thread.is_alive():
            self.logger.info("æ­£åœ¨åœæ­¢SQLiteå†™å…¥çº¿ç¨‹...")
            self._stop_event.set()
            self._write_queue.put(("stop", None))  # å‘é€åœæ­¢ä¿¡å·
            self._write_thread.join(timeout=5.0)
            
            # è¾“å‡ºé˜Ÿåˆ—ç»Ÿè®¡
            stats = self.get_queue_stats()
            self.logger.info(
                f"SQLiteå†™å…¥çº¿ç¨‹å·²åœæ­¢ï¼Œé˜Ÿåˆ—å‰©ä½™: {stats['queue_size']}æ¡ï¼Œ"
                f"çŠ¶æ€: {stats['health_status']}"
            )
    
    @contextmanager
    def _get_conn(self, db_file: Path):
        """
        è·å–æ•°æ®åº“è¿æ¥ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            db_file: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        
        Yields:
            æ•°æ®åº“è¿æ¥
        """
        max_retries = 3
        retry_delay = 0.1  # 100ms
        
        for attempt in range(max_retries):
            try:
                # è®¾ç½®è¶…æ—¶ä¸º30ç§’ï¼ˆé¿å…é•¿æ—¶é—´é”å®šï¼‰
                conn = sqlite3.connect(str(db_file), timeout=30.0, check_same_thread=False)
                try:
                    yield conn
                    conn.commit()
                    return
                except Exception as e:
                    conn.rollback()
                    self.logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {e}", exc_info=True)
                    raise
                finally:
                    conn.close()
            except sqlite3.OperationalError as e:
                if "locked" in str(e) and attempt < max_retries - 1:
                    import time
                    time.sleep(retry_delay * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    continue
                else:
                    raise
    
    def save_ticks(self, df: pd.DataFrame) -> None:
        """
        æ‰¹é‡ä¿å­˜Tickæ•°æ®åˆ°SQLiteï¼ˆå¼‚æ­¥ï¼ŒåŠ å…¥å†™å…¥é˜Ÿåˆ—ï¼‰
        
        Args:
            df: Tickæ•°æ®DataFrame
            
        Note:
            Tickæ•°æ®ä¸å…è®¸ä¸¢å¼ƒï¼é‡‡ç”¨æ— é™å¤§å°é˜Ÿåˆ—ï¼ŒåŠ¨æ€æ‰©å®¹ï¼Œé…åˆåˆ†çº§å‘Šè­¦
        """
        if df.empty:
            return
        
        try:
            # æ£€æŸ¥é˜Ÿåˆ—å¥åº·çŠ¶æ€ï¼ˆè§¦å‘å‘Šè­¦ä½†ä¸é˜»å¡ï¼‰
            self._check_queue_health()
            
            # å°†å†™å…¥ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—ï¼ˆæ— é™å¤§å°é˜Ÿåˆ—ï¼Œä¸ä¼šé˜»å¡ï¼‰
            self._write_queue.put(("tick", df.copy()), block=False)
            
        except Exception as e:
            self.logger.critical(
                f"âŒ FATAL: åŠ å…¥Tickå†™å…¥é˜Ÿåˆ—å¤±è´¥: {e}ï¼Œ"
                f"å½“å‰é˜Ÿåˆ—å¤§å°: {self._write_queue.qsize()}",
                exc_info=True
            )
            raise  # æŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚çŸ¥é“æœ‰ä¸¥é‡é—®é¢˜
    
    def _do_write_ticks(self, df: pd.DataFrame) -> None:
        """
        å®é™…æ‰§è¡ŒTickæ•°æ®å†™å…¥ï¼ˆåœ¨å†™å…¥çº¿ç¨‹ä¸­è°ƒç”¨ï¼ŒæŒ‰åˆçº¦+äº¤æ˜“æ—¥åˆ†åº“ï¼‰
        
        Args:
            df: Tickæ•°æ®DataFrame
        """
        try:
            # æ•°æ®æ¸…æ´—ï¼šç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            required_cols = ["instrument_id", "exchange_id", "datetime", "trading_day"]
            if not all(col in df.columns for col in required_cols):
                self.logger.warning(f"Tickæ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {required_cols}")
                return
            
            # æŒ‰åˆçº¦å’Œäº¤æ˜“æ—¥åˆ†ç»„
            grouped = df.groupby(["instrument_id", "trading_day"])
            
            for (instrument_id, trading_day), group_df in grouped:
                # è·å–è¯¥åˆçº¦+äº¤æ˜“æ—¥çš„æ•°æ®åº“æ–‡ä»¶è·¯å¾„
                db_path = self._get_db_path("tick", str(instrument_id), str(trading_day))
                
                try:
                    conn = sqlite3.connect(str(db_path), timeout=30.0, check_same_thread=False)
                    try:
                        # åˆå§‹åŒ–è¡¨ï¼ˆå¦‚æœæ˜¯æ–°æ•°æ®åº“ï¼‰
                        self._init_tick_table(conn)
                        
                        # å†™å…¥æ•°æ®
                        group_df.to_sql('ticks', conn, if_exists='append', index=False)
                        conn.commit()
                        
                        self.logger.debug(
                            f"âœ“ Tickæ•°æ®å†™å…¥æˆåŠŸ: {instrument_id} @ {trading_day} "
                            f"({len(group_df)}æ¡) -> {db_path.name}"
                        )
                    except Exception as e:
                        conn.rollback()
                        self.logger.error(f"å†™å…¥Tickæ•°æ®å¤±è´¥ [{instrument_id}@{trading_day}]: {e}", exc_info=True)
                    finally:
                        conn.close()
                        
                except Exception as e:
                    self.logger.error(f"è¿æ¥Tickæ•°æ®åº“å¤±è´¥ [{instrument_id}@{trading_day}]: {e}", exc_info=True)
                    
        except Exception as e:
            self.logger.error(f"Tickæ•°æ®åˆ†ç»„å†™å…¥å¤±è´¥: {e}", exc_info=True)
    
    def save_klines(self, df: pd.DataFrame) -> None:
        """
        æ‰¹é‡ä¿å­˜Kçº¿æ•°æ®åˆ°SQLiteï¼ˆå¼‚æ­¥ï¼ŒåŠ å…¥å†™å…¥é˜Ÿåˆ—ï¼‰
        
        Args:
            df: Kçº¿æ•°æ®DataFrame
            
        Note:
            ä½¿ç”¨æ— é™å¤§å°é˜Ÿåˆ—ï¼ŒåŠ¨æ€æ‰©å®¹
        """
        if df.empty:
            return
        
        try:
            # æ£€æŸ¥é˜Ÿåˆ—å¥åº·çŠ¶æ€
            self._check_queue_health()
            
            # å°†å†™å…¥ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—ï¼ˆæ— é™å¤§å°é˜Ÿåˆ—ï¼Œä¸ä¼šé˜»å¡ï¼‰
            self._write_queue.put(("kline", df.copy()), block=False)
            
        except Exception as e:
            self.logger.error(
                f"åŠ å…¥Kçº¿å†™å…¥é˜Ÿåˆ—å¤±è´¥: {e}ï¼Œ"
                f"å½“å‰é˜Ÿåˆ—å¤§å°: {self._write_queue.qsize()}",
                exc_info=True
            )
    
    def _do_write_klines(self, df: pd.DataFrame) -> None:
        """
        å®é™…æ‰§è¡ŒKçº¿æ•°æ®å†™å…¥ï¼ˆåœ¨å†™å…¥çº¿ç¨‹ä¸­è°ƒç”¨ï¼ŒæŒ‰åˆçº¦+äº¤æ˜“æ—¥åˆ†åº“ï¼‰
        
        Args:
            df: Kçº¿æ•°æ®DataFrame
        """
        try:
            # æ•°æ®æ¸…æ´—ï¼šç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            required_cols = ["instrument_id", "exchange_id", "interval", "datetime", "trading_day"]
            if not all(col in df.columns for col in required_cols):
                self.logger.warning(f"Kçº¿æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {required_cols}")
                return
            
            # æŒ‰åˆçº¦å’Œäº¤æ˜“æ—¥åˆ†ç»„
            grouped = df.groupby(["instrument_id", "trading_day"])
            
            for (instrument_id, trading_day), group_df in grouped:
                # è·å–è¯¥åˆçº¦+äº¤æ˜“æ—¥çš„æ•°æ®åº“æ–‡ä»¶è·¯å¾„
                db_path = self._get_db_path("kline", str(instrument_id), str(trading_day))
                
                try:
                    conn = sqlite3.connect(str(db_path), timeout=30.0, check_same_thread=False)
                    try:
                        # åˆå§‹åŒ–è¡¨ï¼ˆå¦‚æœæ˜¯æ–°æ•°æ®åº“ï¼‰
                        self._init_kline_table(conn)
                        
                        # å†™å…¥æ•°æ®
                        group_df.to_sql('klines', conn, if_exists='append', index=False)
                        conn.commit()
                        
                        self.logger.debug(
                            f"âœ“ Kçº¿æ•°æ®å†™å…¥æˆåŠŸ: {instrument_id} @ {trading_day} "
                            f"({len(group_df)}æ¡) -> {db_path.name}"
                        )
                    except Exception as e:
                        conn.rollback()
                        self.logger.error(f"å†™å…¥Kçº¿æ•°æ®å¤±è´¥ [{instrument_id}@{trading_day}]: {e}", exc_info=True)
                    finally:
                        conn.close()
                        
                except Exception as e:
                    self.logger.error(f"è¿æ¥Kçº¿æ•°æ®åº“å¤±è´¥ [{instrument_id}@{trading_day}]: {e}", exc_info=True)
                    
        except Exception as e:
            self.logger.error(f"Kçº¿æ•°æ®åˆ†ç»„å†™å…¥å¤±è´¥: {e}", exc_info=True)
    
    def query_ticks(self,
                    instrument_id: str,
                    start_time: str,
                    end_time: str) -> pd.DataFrame:
        """
        æŸ¥è¯¢Tickæ•°æ®
        
        Args:
            instrument_id: åˆçº¦ä»£ç 
            start_time: å¼€å§‹æ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
        
        Returns:
            Tickæ•°æ®DataFrame
        """
        try:
            with self._get_conn(self.tick_db_file) as conn:
                query = """
                    SELECT * FROM ticks
                    WHERE instrument_id = ?
                    AND datetime >= ?
                    AND datetime <= ?
                    ORDER BY datetime
                """
                df = pd.read_sql_query(
                    query, 
                    conn, 
                    params=(instrument_id, start_time, end_time)
                )
                self.logger.debug(f"æŸ¥è¯¢åˆ° {len(df)} æ¡Tickæ•°æ®")
                return df
        
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢Tickæ•°æ®å¤±è´¥: {e}", exc_info=True)
            return pd.DataFrame()
    
    def query_klines(self,
                     instrument_id: str,
                     interval: str,
                     start_time: str,
                     end_time: str) -> pd.DataFrame:
        """
        æŸ¥è¯¢Kçº¿æ•°æ®
        
        Args:
            instrument_id: åˆçº¦ä»£ç 
            interval: Kçº¿å‘¨æœŸ
            start_time: å¼€å§‹æ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
        
        Returns:
            Kçº¿æ•°æ®DataFrame
        """
        try:
            with self._get_conn(self.kline_db_file) as conn:
                query = """
                    SELECT * FROM klines
                    WHERE instrument_id = ?
                    AND interval = ?
                    AND datetime >= ?
                    AND datetime <= ?
                    ORDER BY datetime
                """
                df = pd.read_sql_query(
                    query, 
                    conn, 
                    params=(instrument_id, interval, start_time, end_time)
                )
                self.logger.debug(f"æŸ¥è¯¢åˆ° {len(df)} æ¡Kçº¿æ•°æ®")
                return df
        
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢Kçº¿æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return pd.DataFrame()
    
    def get_archivable_data(self, cutoff_date: str) -> dict:
        """
        è·å–å¯å½’æ¡£çš„æ•°æ®ï¼ˆè¶…è¿‡ä¿ç•™æœŸçš„æ•°æ®ï¼‰
        
        Args:
            cutoff_date: æˆªæ­¢æ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD
        
        Returns:
            {"ticks": DataFrame, "klines": DataFrame}
        """
        result = {}
        
        try:
            # æŸ¥è¯¢Tickæ•°æ®
            with self._get_conn(self.tick_db_file) as conn:
                query = "SELECT * FROM ticks WHERE datetime < ?"
                result['ticks'] = pd.read_sql_query(query, conn, params=(cutoff_date,))
            
            # æŸ¥è¯¢Kçº¿æ•°æ®
            with self._get_conn(self.kline_db_file) as conn:
                query = "SELECT * FROM klines WHERE datetime < ?"
                result['klines'] = pd.read_sql_query(query, conn, params=(cutoff_date,))
            
            self.logger.info(
                f"è·å–å¯å½’æ¡£æ•°æ®: Tick={len(result['ticks'])}æ¡, Kçº¿={len(result['klines'])}æ¡"
            )
            return result
        
        except Exception as e:
            self.logger.error(f"è·å–å¯å½’æ¡£æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return {"ticks": pd.DataFrame(), "klines": pd.DataFrame()}
    
    def delete_archived_data(self, cutoff_date: str) -> None:
        """
        åˆ é™¤å·²å½’æ¡£çš„æ•°æ®
        
        Args:
            cutoff_date: æˆªæ­¢æ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD
        """
        try:
            # åˆ é™¤Tickæ•°æ®
            with self._get_conn(self.tick_db_file) as conn:
                cursor = conn.execute("DELETE FROM ticks WHERE datetime < ?", (cutoff_date,))
                tick_count = cursor.rowcount
                self.logger.info(f"åˆ é™¤ {tick_count} æ¡å·²å½’æ¡£Tickæ•°æ®")
            
            # åˆ é™¤Kçº¿æ•°æ®
            with self._get_conn(self.kline_db_file) as conn:
                cursor = conn.execute("DELETE FROM klines WHERE datetime < ?", (cutoff_date,))
                kline_count = cursor.rowcount
                self.logger.info(f"åˆ é™¤ {kline_count} æ¡å·²å½’æ¡£Kçº¿æ•°æ®")
            
            # æ‰§è¡ŒVACUUMä»¥å›æ”¶ç©ºé—´
            self._vacuum_databases()
        
        except Exception as e:
            self.logger.error(f"åˆ é™¤å·²å½’æ¡£æ•°æ®å¤±è´¥: {e}", exc_info=True)
    
    def _vacuum_databases(self) -> None:
        """å‹ç¼©æ•°æ®åº“ï¼Œå›æ”¶ç©ºé—´"""
        try:
            with self._get_conn(self.tick_db_file) as conn:
                conn.execute("VACUUM")
            
            with self._get_conn(self.kline_db_file) as conn:
                conn.execute("VACUUM")
            
            self.logger.info("æ•°æ®åº“å‹ç¼©å®Œæˆ")
        
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“å‹ç¼©å¤±è´¥: {e}", exc_info=True)
    
    def get_statistics(self) -> dict:
        """
        è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            "retention_days": self.retention_days,
            "tick_count": 0,
            "kline_count": 0,
            "db_size_mb": 0.0
        }
        
        try:
            # ç»Ÿè®¡Tickæ•°é‡
            with self._get_conn(self.tick_db_file) as conn:
                cursor = conn.execute("SELECT COUNT(*) FROM ticks")
                stats["tick_count"] = cursor.fetchone()[0]
            
            # ç»Ÿè®¡Kçº¿æ•°é‡
            with self._get_conn(self.kline_db_file) as conn:
                cursor = conn.execute("SELECT COUNT(*) FROM klines")
                stats["kline_count"] = cursor.fetchone()[0]
            
            # ç»Ÿè®¡æ•°æ®åº“æ–‡ä»¶å¤§å°
            if self.tick_db_file.exists():
                stats["db_size_mb"] += self.tick_db_file.stat().st_size / (1024 * 1024)
            if self.kline_db_file.exists():
                stats["db_size_mb"] += self.kline_db_file.stat().st_size / (1024 * 1024)
            
            stats["db_size_mb"] = round(stats["db_size_mb"], 2)
        
        except Exception as e:
            self.logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
        
        return stats

