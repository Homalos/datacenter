#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@ProjectName: homalos-datacenter
@FileName   : sqlite_storage.py
@Date       : 2025/10/27
@Author     : Lumosylva
@Email      : donnymoving@gmail.com
@Software   : PyCharm
@Description: SQLiteå­˜å‚¨å±‚ - ç”¨äºè¿‘æœŸæ•°æ®çš„å¿«é€ŸæŸ¥è¯¢
"""
import sqlite3
import pandas as pd  # type: ignore
import threading
import queue
import time
from pathlib import Path
from contextlib import contextmanager

from src.utils.log import get_logger


class SQLiteStorage:
    """
    SQLiteå­˜å‚¨å±‚ - ç”¨äºè¿‘æœŸæ•°æ®çš„å¿«é€ŸæŸ¥è¯¢
    
    ç‰¹ç‚¹ï¼š
    1. é«˜é€Ÿè¯»å†™ï¼šWALæ¨¡å¼ï¼Œæ”¯æŒå¹¶å‘
    2. ç´¢å¼•ä¼˜åŒ–ï¼šåˆçº¦ä»£ç +æ—¶é—´çš„å¤åˆç´¢å¼•
    3. è‡ªåŠ¨æ¸…ç†ï¼šå®šæœŸåˆ é™¤å·²å½’æ¡£æ•°æ®
    4. è½»é‡çº§ï¼šå•æ–‡ä»¶å­˜å‚¨ï¼Œæ˜“äºå¤‡ä»½
    """
    
    def __init__(self, 
                 db_path: str = "data/db",
                 retention_days: int = 7,
                 trading_day_manager = None):
        """
        åˆå§‹åŒ–SQLiteå­˜å‚¨å±‚ï¼ˆæŒ‰äº¤æ˜“æ—¥+åˆçº¦åˆ†åº“å­˜å‚¨ï¼‰
        
        Args:
            db_path: æ•°æ®åº“æ–‡ä»¶æ ¹ç›®å½•
            retention_days: æ•°æ®ä¿ç•™å¤©æ•°ï¼ˆé»˜è®¤7å¤©ï¼‰
            trading_day_manager: äº¤æ˜“æ—¥ç®¡ç†å™¨
            
        æ•°æ®åº“æ–‡ä»¶ç»“æ„ï¼š
            - data/db/tick/20251027/SA601.db
            - data/db/kline/20251027/SA601.db
        """
        self.db_root = Path(db_path)
        self.db_root.mkdir(parents=True, exist_ok=True)
        
        self.tick_db_root = self.db_root / "tick"
        self.kline_db_root = self.db_root / "kline"
        self.tick_db_root.mkdir(parents=True, exist_ok=True)
        self.kline_db_root.mkdir(parents=True, exist_ok=True)
        
        self.logger = get_logger(self.__class__.__name__)
        self.retention_days = retention_days
        self.trading_day_manager = trading_day_manager
        
        # å‘åå…¼å®¹å±æ€§ï¼ˆæŸ¥è¯¢æ–¹æ³•æš‚æœªé‡æ„ä¸ºåˆ†åº“æŸ¥è¯¢ï¼‰
        # TODO: é‡æ„æŸ¥è¯¢æ–¹æ³•ä»¥æ”¯æŒåˆ†åº“æŸ¥è¯¢
        self.tick_db_file = self.tick_db_root / "deprecated_single_file.db"
        self.kline_db_file = self.kline_db_root / "deprecated_single_file.db"
        
        # å†™å…¥é˜Ÿåˆ—ï¼ˆè§£å†³å¹¶å‘é”é—®é¢˜ï¼Œæ”¯æŒåŠ¨æ€æ‰©å®¹ï¼‰
        self._write_queue: "queue.Queue[tuple]" = queue.Queue(maxsize=0)  # æ— é™å¤§å°ï¼ŒåŠ¨æ€æ‰©å®¹
        self._write_thread: threading.Thread | None = None
        self._stop_event = threading.Event()
        
        # é˜Ÿåˆ—ç›‘æ§é˜ˆå€¼
        self._queue_warn_threshold = 5000   # è½¯æ€§å‘Šè­¦é˜ˆå€¼
        self._queue_critical_threshold = 20000  # ä¸¥é‡å‘Šè­¦é˜ˆå€¼
        self._last_queue_warn_time = 0.0  # ä¸Šæ¬¡å‘Šè­¦æ—¶é—´ï¼ˆé¿å…æ—¥å¿—åˆ·å±ï¼‰
        
        # å¯åŠ¨å†™å…¥çº¿ç¨‹
        self._start_write_thread()
        
        self.logger.info(
            f"SQLiteå­˜å‚¨å±‚åˆå§‹åŒ–å®Œæˆï¼ˆæŒ‰äº¤æ˜“æ—¥+åˆçº¦åˆ†åº“ï¼ŒåŠ¨æ€æ‰©å®¹é˜Ÿåˆ—ï¼‰ï¼Œ"
            f"æ•°æ®ä¿ç•™{retention_days}å¤©ï¼Œå·²å¯åŠ¨å•çº¿ç¨‹å†™å…¥é˜Ÿåˆ—"
        )
    
    def _get_db_path(self, data_type: str, instrument_id: str, trading_day: str) -> Path:
        """
        è·å–æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼ˆæŒ‰äº¤æ˜“æ—¥+åˆçº¦åˆ†åº“ï¼‰
        
        Args:
            data_type: æ•°æ®ç±»å‹ï¼ˆ'tick' æˆ– 'kline'ï¼‰
            instrument_id: åˆçº¦ä»£ç 
            trading_day: äº¤æ˜“æ—¥ï¼ˆYYYYMMDDï¼‰
            
        Returns:
            æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼ˆä¾‹å¦‚ï¼šdata/db/tick/20251027/SA601.dbï¼‰
        """
        if data_type == "tick":
            root = self.tick_db_root
        elif data_type == "kline":
            root = self.kline_db_root
        else:
            raise ValueError(f"æœªçŸ¥çš„æ•°æ®ç±»å‹: {data_type}")
        
        # data/db/tick/20251027/
        day_dir = root / trading_day
        day_dir.mkdir(parents=True, exist_ok=True)
        
        # data/db/tick/20251027/SA601.db
        return day_dir / f"{instrument_id}.db"
    
    def _get_trading_days_between(self, data_type: str, start_time: str, end_time: str) -> list[str]:
        """
        æšä¸¾æ—¶é—´èŒƒå›´å†…çš„æ‰€æœ‰äº¤æ˜“æ—¥ç›®å½•
        
        Args:
            data_type: æ•°æ®ç±»å‹ï¼ˆ'tick' æˆ– 'kline'ï¼‰
            start_time: å¼€å§‹æ—¶é—´ï¼ˆISOæ ¼å¼ï¼Œå¦‚ '2025-10-27 14:00:00'ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆISOæ ¼å¼ï¼Œå¦‚ '2025-10-28 16:00:00'ï¼‰
        
        Returns:
            äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆYYYYMMDDæ ¼å¼ï¼Œå‡åºæ’åˆ—ï¼‰
        
        Note:
            æ‰«æå®é™…å­˜åœ¨çš„äº¤æ˜“æ—¥ç›®å½•ï¼Œè€Œä¸æ˜¯è®¡ç®—ç†è®ºæ—¥æœŸèŒƒå›´
        """
        try:
            # ç¡®å®šæ•°æ®åº“æ ¹ç›®å½•
            if data_type == "tick":
                root = self.tick_db_root
            elif data_type == "kline":
                root = self.kline_db_root
            else:
                return []
            
            # æ‰«ææ‰€æœ‰äº¤æ˜“æ—¥ç›®å½•ï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰
            trading_days = []
            if root.exists():
                for day_dir in root.iterdir():
                    if day_dir.is_dir() and day_dir.name.isdigit() and len(day_dir.name) == 8:
                        trading_days.append(day_dir.name)
            
            # æŒ‰æ—¥æœŸæ’åº
            trading_days.sort()
            
            # è¿‡æ»¤ï¼šåªä¿ç•™æ—¶é—´èŒƒå›´å†…çš„äº¤æ˜“æ—¥
            from datetime import datetime, timedelta
            
            try:
                # æå–æ—¥æœŸéƒ¨åˆ†ï¼ˆYYYY-MM-DD æ ¼å¼ï¼‰
                start_date_str = start_time.split()[0] if ' ' in start_time else start_time[:10]
                end_date_str = end_time.split()[0] if ' ' in end_time else end_time[:10]
                
                # è§£æä¸º datetime å¯¹è±¡
                start_date = datetime.fromisoformat(start_date_str)
                end_date = datetime.fromisoformat(end_date_str)
            except Exception as e:
                # å¦‚æœæ—¶é—´è§£æå¤±è´¥ï¼Œè¿”å›æ‰€æœ‰äº¤æ˜“æ—¥ï¼ˆå®‰å…¨å›é€€ï¼‰
                self.logger.warning(f"æ—¶é—´è§£æå¤±è´¥: {e}ï¼Œè¿”å›æ‰€æœ‰äº¤æ˜“æ—¥")
                return trading_days
            
            # è¿‡æ»¤äº¤æ˜“æ—¥ï¼ˆå®½æ¾ç­–ç•¥ï¼šå‰åå„å¤šæŸ¥1å¤©ï¼Œé˜²æ­¢å¤œç›˜æ•°æ®é—æ¼ï¼‰
            filtered_days = []
            for day_str in trading_days:
                try:
                    # å°†äº¤æ˜“æ—¥å­—ç¬¦ä¸²ï¼ˆYYYYMMDDï¼‰è½¬æ¢ä¸º datetime
                    day_date = datetime.strptime(day_str, "%Y%m%d")
                    
                    # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼ˆå®½æ¾ï¼šstart_date - 1å¤© åˆ° end_date + 1å¤©ï¼‰
                    # è¿™æ ·å¯ä»¥è¦†ç›–å¤œç›˜æ•°æ®ï¼ˆå¤œç›˜å±äºä¸‹ä¸€äº¤æ˜“æ—¥ï¼‰
                    range_start = start_date - timedelta(days=1)
                    range_end = end_date + timedelta(days=1)
                    
                    if range_start <= day_date <= range_end:
                        filtered_days.append(day_str)
                except Exception as e:
                    self.logger.debug(f"è§£æäº¤æ˜“æ—¥å¤±è´¥ {day_str}: {e}")
                    continue
            
            return filtered_days
        
        except Exception as e:
            self.logger.warning(f"æšä¸¾äº¤æ˜“æ—¥ç›®å½•å¤±è´¥: {e}ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []

    @staticmethod
    def _init_tick_table(conn) -> None:
        """
        åˆå§‹åŒ–Tickè¡¨ç»“æ„ï¼ˆ45ä¸ªå­—æ®µï¼ŒPascalCaseå‘½åï¼‰
        
        å­—æ®µé¡ºåºï¼ˆæŒ‰ç”¨æˆ·æŒ‡å®šï¼‰ï¼š
        1. TradingDay: äº¤æ˜“æ—¥
        2. ExchangeID: äº¤æ˜“æ‰€ä»£ç 
        3. LastPrice: æœ€æ–°ä»·
        4. PreSettlementPrice: æ˜¨ç»“ç®—ä»·
        5. PreClosePrice: æ˜¨æ”¶ç›˜ä»·
        6. PreOpenInterest: æ˜¨æŒä»“é‡
        7. OpenPrice: å¼€ç›˜ä»·
        8. HighestPrice: æœ€é«˜ä»·
        9. LowestPrice: æœ€ä½ä»·
        10. Volume: æˆäº¤é‡
        11. Turnover: æˆäº¤é¢
        12. OpenInterest: æŒä»“é‡
        13. ClosePrice: æ”¶ç›˜ä»·
        14. SettlementPrice: ç»“ç®—ä»·
        15. UpperLimitPrice: æ¶¨åœä»·
        16. LowerLimitPrice: è·Œåœä»·
        17. PreDelta: æ˜¨è™šå®åº¦
        18. CurrDelta: ä»Šè™šå®åº¦
        19. UpdateTime: æ›´æ–°æ—¶é—´
        20. UpdateMillisec: æ›´æ–°æ¯«ç§’
        21-40. äº”æ¡£ä¹°å–ç›˜å£
        41. AveragePrice: å‡ä»·
        42. ActionDay: å®é™…æ—¥æœŸ
        43. InstrumentID: åˆçº¦ä»£ç 
        44. ExchangeInstID: äº¤æ˜“æ‰€åˆçº¦ä»£ç 
        45. BandingUpperPrice: æ³¢åŠ¨ä¸Šé™
        46. BandingLowerPrice: æ³¢åŠ¨ä¸‹é™
        47. Timestamp: å®Œæ•´æ—¶é—´æˆ³
        """
        with conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS ticks (
                    ID INTEGER PRIMARY KEY AUTOINCREMENT,
                    TradingDay TEXT,
                    ExchangeID TEXT,
                    LastPrice REAL,
                    PreSettlementPrice REAL,
                    PreClosePrice REAL,
                    PreOpenInterest REAL,
                    OpenPrice REAL,
                    HighestPrice REAL,
                    LowestPrice REAL,
                    Volume INTEGER,
                    Turnover REAL,
                    OpenInterest REAL,
                    ClosePrice REAL,
                    SettlementPrice REAL,
                    UpperLimitPrice REAL,
                    LowerLimitPrice REAL,
                    PreDelta REAL,
                    CurrDelta REAL,
                    UpdateTime TEXT,
                    UpdateMillisec INTEGER,
                    BidPrice1 REAL,
                    BidVolume1 INTEGER,
                    AskPrice1 REAL,
                    AskVolume1 INTEGER,
                    BidPrice2 REAL,
                    BidVolume2 INTEGER,
                    AskPrice2 REAL,
                    AskVolume2 INTEGER,
                    BidPrice3 REAL,
                    BidVolume3 INTEGER,
                    AskPrice3 REAL,
                    AskVolume3 INTEGER,
                    BidPrice4 REAL,
                    BidVolume4 INTEGER,
                    AskPrice4 REAL,
                    AskVolume4 INTEGER,
                    BidPrice5 REAL,
                    BidVolume5 INTEGER,
                    AskPrice5 REAL,
                    AskVolume5 INTEGER,
                    AveragePrice REAL,
                    ActionDay TEXT,
                    InstrumentID TEXT NOT NULL,
                    ExchangeInstID TEXT,
                    BandingUpperPrice REAL,
                    BandingLowerPrice REAL,
                    Timestamp TIMESTAMP NOT NULL,
                    CreatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # åˆ›å»ºç´¢å¼•ï¼ˆæŒ‰åˆçº¦ã€äº¤æ˜“æ—¥å’Œæ—¶é—´æŸ¥è¯¢ï¼‰
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_tick_instrument_time
                ON ticks(InstrumentID, TradingDay, Timestamp)
            """)
            
            # å¯ç”¨WALæ¨¡å¼ï¼ˆæé«˜å¹¶å‘æ€§èƒ½ï¼‰
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=10000")

    @staticmethod
    def _init_kline_table(conn) -> None:
        """
        åˆå§‹åŒ–Kçº¿è¡¨ç»“æ„ï¼ˆç²¾ç®€ç‰ˆï¼ŒåŒ…å«13ä¸ªæ ¸å¿ƒå­—æ®µï¼ŒPascalCaseå‘½åï¼‰
        
        å­—æ®µè¯´æ˜ï¼ˆæŒ‰è¡¨é¡ºåºï¼‰ï¼š
        - ID: è‡ªå¢ä¸»é”®
        - BarType: Kçº¿ç±»å‹/å‘¨æœŸ
        - TradingDay: äº¤æ˜“æ—¥ï¼ˆç”¨äºåˆ†åº“ï¼‰
        - UpdateTime: æœ€åæ›´æ–°æ—¶é—´
        - InstrumentID: åˆçº¦ä»£ç 
        - ExchangeID: äº¤æ˜“æ‰€ä»£ç 
        - Volume: æˆäº¤é‡
        - OpenInterest: æŒä»“é‡
        - OpenPrice: å¼€ç›˜ä»·
        - HighestPrice: æœ€é«˜ä»·
        - LowestPrice: æœ€ä½ä»·
        - ClosePrice: æ”¶ç›˜ä»·
        - LastVolume: Kçº¿å¼€å§‹æ—¶çš„ç´¯è®¡æˆäº¤é‡
        - Timestamp: Kçº¿å¼€å§‹æ—¶é—´ï¼ˆå®Œæ•´datetimeï¼Œç”¨äºæ—¶é—´èŒƒå›´æŸ¥è¯¢ï¼‰
        - CreatedAt: è®°å½•åˆ›å»ºæ—¶é—´
        """
        with conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS klines (
                    ID INTEGER PRIMARY KEY AUTOINCREMENT,
                    BarType TEXT NOT NULL,
                    TradingDay TEXT,
                    UpdateTime TEXT,
                    InstrumentID TEXT NOT NULL,
                    ExchangeID TEXT NOT NULL,
                    Volume INTEGER,
                    OpenInterest REAL,
                    OpenPrice REAL,
                    HighestPrice REAL,
                    LowestPrice REAL,
                    ClosePrice REAL,
                    LastVolume INTEGER,
                    Timestamp TIMESTAMP NOT NULL,
                    CreatedAt TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # åˆ›å»ºç´¢å¼•ï¼ˆæŒ‰åˆçº¦ã€Kçº¿ç±»å‹å’Œæ—¶é—´æŸ¥è¯¢ï¼Œä¼˜åŒ–æ—¶é—´èŒƒå›´è¿‡æ»¤ï¼‰
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_kline_instrument_time
                ON klines(InstrumentID, BarType, Timestamp)
            """)
            
            # å¯ç”¨WALæ¨¡å¼
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=10000")
    
    def _check_queue_health(self) -> None:
        """
        æ£€æŸ¥é˜Ÿåˆ—å¥åº·çŠ¶æ€å¹¶è§¦å‘å‘Šè­¦
        
        é‡‡ç”¨åˆ†çº§å‘Šè­¦æœºåˆ¶ï¼š
        - 5000æ¡ï¼šè½¯æ€§å‘Šè­¦ï¼ˆ1åˆ†é’Ÿæœ€å¤š1æ¬¡ï¼‰
        - 20000æ¡ï¼šä¸¥é‡å‘Šè­¦ï¼ˆç«‹å³å‘Šè­¦ï¼‰
        """
        queue_size = self._write_queue.qsize()
        current_time = time.time()
        
        # ä¸¥é‡å‘Šè­¦ï¼ˆè¶…è¿‡20000æ¡ï¼‰
        if queue_size >= self._queue_critical_threshold:
            self.logger.error(
                f"ğŸ”´ CRITICAL: SQLiteå†™å…¥é˜Ÿåˆ—ä¸¥é‡ç§¯å‹ï¼"
                f"å½“å‰: {queue_size}æ¡ï¼ˆè¶…è¿‡ä¸¥é‡é˜ˆå€¼{self._queue_critical_threshold}ï¼‰ï¼Œ"
                f"å¯èƒ½å­˜åœ¨æ€§èƒ½ç“¶é¢ˆï¼Œè¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€ï¼"
            )
            self._last_queue_warn_time = current_time
            
        # è½¯æ€§å‘Šè­¦ï¼ˆè¶…è¿‡5000æ¡ï¼Œä¸”è·ç¦»ä¸Šæ¬¡å‘Šè­¦è¶…è¿‡60ç§’ï¼‰
        elif queue_size >= self._queue_warn_threshold:
            if current_time - self._last_queue_warn_time > 60:  # 60ç§’å†…åªå‘Šè­¦ä¸€æ¬¡
                self.logger.warning(
                    f"âš ï¸  SQLiteå†™å…¥é˜Ÿåˆ—ç§¯å‹: {queue_size}æ¡ "
                    f"(å‘Šè­¦é˜ˆå€¼: {self._queue_warn_threshold})"
                )
                self._last_queue_warn_time = current_time
    
    def _start_write_thread(self) -> None:
        """å¯åŠ¨å•ç‹¬çš„å†™å…¥çº¿ç¨‹ï¼ˆè§£å†³å¹¶å‘é”é—®é¢˜ï¼‰"""
        if self._write_thread is None or not self._write_thread.is_alive():
            self._stop_event.clear()
            self._write_thread = threading.Thread(
                target=self._write_worker,
                name="SQLiteWriteThread",
                daemon=True
            )
            self._write_thread.start()
            self.logger.info("SQLiteå†™å…¥çº¿ç¨‹å·²å¯åŠ¨")
    
    def _write_worker(self) -> None:
        """å†™å…¥çº¿ç¨‹å·¥ä½œå‡½æ•° - ä»é˜Ÿåˆ—ä¸­å–ä»»åŠ¡å¹¶ä¸²è¡Œå†™å…¥"""
        self.logger.info("SQLiteå†™å…¥çº¿ç¨‹å¼€å§‹å·¥ä½œ...")
        
        while not self._stop_event.is_set():
            try:
                # ä»é˜Ÿåˆ—è·å–å†™å…¥ä»»åŠ¡ï¼ˆè®¾ç½®è¶…æ—¶ä»¥ä¾¿æ£€æŸ¥stop_eventï¼‰
                task = self._write_queue.get(timeout=1.0)
                
                if task[0] == "stop":  # åœæ­¢ä¿¡å·
                    break
                
                # æ‰§è¡Œå†™å…¥ä»»åŠ¡
                task_type, args = task
                try:
                    if task_type == "tick":
                        self._do_write_ticks(args)
                    elif task_type == "kline":
                        self._do_write_klines(args)
                except Exception as e:
                    self.logger.error(f"å†™å…¥ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
                finally:
                    self._write_queue.task_done()
                    
            except queue.Empty:
                continue
            except Exception as e:
                self.logger.error(f"å†™å…¥çº¿ç¨‹å¼‚å¸¸: {e}", exc_info=True)
        
        self.logger.info("SQLiteå†™å…¥çº¿ç¨‹å·²åœæ­¢")
    
    def get_queue_stats(self) -> dict:
        """
        è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«é˜Ÿåˆ—çŠ¶æ€çš„å­—å…¸
        """
        queue_size = self._write_queue.qsize()
        return {
            "queue_size": queue_size,
            "warn_threshold": self._queue_warn_threshold,
            "critical_threshold": self._queue_critical_threshold,
            "health_status": (
                "critical" if queue_size >= self._queue_critical_threshold
                else "warning" if queue_size >= self._queue_warn_threshold
                else "healthy"
            ),
            "usage_percent": round(queue_size / self._queue_warn_threshold * 100, 2) if queue_size > 0 else 0
        }
    
    def stop(self) -> None:
        """åœæ­¢å†™å…¥çº¿ç¨‹"""
        if self._write_thread and self._write_thread.is_alive():
            self.logger.info("æ­£åœ¨åœæ­¢SQLiteå†™å…¥çº¿ç¨‹...")
            self._stop_event.set()
            self._write_queue.put(("stop", None))  # å‘é€åœæ­¢ä¿¡å·
            self._write_thread.join(timeout=5.0)
            
            # è¾“å‡ºé˜Ÿåˆ—ç»Ÿè®¡
            stats = self.get_queue_stats()
            self.logger.info(
                f"SQLiteå†™å…¥çº¿ç¨‹å·²åœæ­¢ï¼Œé˜Ÿåˆ—å‰©ä½™: {stats['queue_size']}æ¡ï¼Œ"
                f"çŠ¶æ€: {stats['health_status']}"
            )
    
    @contextmanager
    def _get_conn(self, db_file: Path):
        """
        è·å–æ•°æ®åº“è¿æ¥ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ”¯æŒé‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            db_file: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        
        Yields:
            æ•°æ®åº“è¿æ¥
        """
        max_retries = 3
        retry_delay = 0.1  # 100ms
        
        for attempt in range(max_retries):
            try:
                # è®¾ç½®è¶…æ—¶ä¸º30ç§’ï¼ˆé¿å…é•¿æ—¶é—´é”å®šï¼‰
                conn = sqlite3.connect(str(db_file), timeout=30.0, check_same_thread=False)
                try:
                    yield conn
                    conn.commit()
                    return
                except Exception as e:
                    conn.rollback()
                    self.logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {e}", exc_info=True)
                    raise
                finally:
                    conn.close()
            except sqlite3.OperationalError as e:
                if "locked" in str(e) and attempt < max_retries - 1:
                    import time
                    time.sleep(retry_delay * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    continue
                else:
                    raise
    
    def save_ticks(self, df: pd.DataFrame) -> None:
        """
        æ‰¹é‡ä¿å­˜Tickæ•°æ®åˆ°SQLiteï¼ˆå¼‚æ­¥ï¼ŒåŠ å…¥å†™å…¥é˜Ÿåˆ—ï¼‰
        
        Args:
            df: Tickæ•°æ®DataFrame
            
        Note:
            Tickæ•°æ®ä¸å…è®¸ä¸¢å¼ƒï¼é‡‡ç”¨æ— é™å¤§å°é˜Ÿåˆ—ï¼ŒåŠ¨æ€æ‰©å®¹ï¼Œé…åˆåˆ†çº§å‘Šè­¦
        """
        if df.empty:
            return
        
        try:
            # æ£€æŸ¥é˜Ÿåˆ—å¥åº·çŠ¶æ€ï¼ˆè§¦å‘å‘Šè­¦ä½†ä¸é˜»å¡ï¼‰
            self._check_queue_health()
            
            # å°†å†™å…¥ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—ï¼ˆæ— é™å¤§å°é˜Ÿåˆ—ï¼Œä¸ä¼šé˜»å¡ï¼‰
            self._write_queue.put(("tick", df.copy()), block=False)
            
        except Exception as e:
            self.logger.critical(
                f"âŒ FATAL: åŠ å…¥Tickå†™å…¥é˜Ÿåˆ—å¤±è´¥: {e}ï¼Œ"
                f"å½“å‰é˜Ÿåˆ—å¤§å°: {self._write_queue.qsize()}",
                exc_info=True
            )
            raise  # æŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚çŸ¥é“æœ‰ä¸¥é‡é—®é¢˜
    
    def _do_write_ticks(self, df: pd.DataFrame) -> None:
        """
        å®é™…æ‰§è¡ŒTickæ•°æ®å†™å…¥ï¼ˆåœ¨å†™å…¥çº¿ç¨‹ä¸­è°ƒç”¨ï¼ŒæŒ‰åˆçº¦+äº¤æ˜“æ—¥åˆ†åº“ï¼‰
        
        Args:
            df: Tickæ•°æ®DataFrameï¼ˆ45ä¸ªå­—æ®µï¼ŒPascalCaseå‘½åï¼‰
        """
        try:
            # æ•°æ®æ¸…æ´—ï¼šç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨ï¼ˆPascalCaseå‘½åï¼‰
            required_cols = ["InstrumentID", "ExchangeID", "Timestamp", "TradingDay"]
            if not all(col in df.columns for col in required_cols):
                self.logger.warning(f"Tickæ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {required_cols}ï¼Œå®é™…å­—æ®µ: {df.columns.tolist()}")
                return
            
            # æŒ‰åˆçº¦å’Œäº¤æ˜“æ—¥åˆ†ç»„
            grouped = df.groupby(["InstrumentID", "TradingDay"])
            
            for (instrument_id, trading_day), group_df in grouped:
                # è·å–è¯¥åˆçº¦+äº¤æ˜“æ—¥çš„æ•°æ®åº“æ–‡ä»¶è·¯å¾„
                db_path = self._get_db_path("tick", str(instrument_id), str(trading_day))
                
                try:
                    conn = sqlite3.connect(str(db_path), timeout=30.0, check_same_thread=False)
                    try:
                        # åˆå§‹åŒ–è¡¨ï¼ˆå¦‚æœæ˜¯æ–°æ•°æ®åº“ï¼‰
                        self._init_tick_table()
                        
                        # å†™å…¥æ•°æ®
                        group_df.to_sql('ticks', conn, if_exists='append', index=False)
                        conn.commit()
                        
                        self.logger.debug(
                            f"âœ“ Tickæ•°æ®å†™å…¥æˆåŠŸ: {instrument_id} @ {trading_day} "
                            f"({len(group_df)}æ¡) -> {db_path.name}"
                        )
                    except Exception as e:
                        conn.rollback()
                        self.logger.error(f"å†™å…¥Tickæ•°æ®å¤±è´¥ [{instrument_id}@{trading_day}]: {e}", exc_info=True)
                    finally:
                        conn.close()
                        
                except Exception as e:
                    self.logger.error(f"è¿æ¥Tickæ•°æ®åº“å¤±è´¥ [{instrument_id}@{trading_day}]: {e}", exc_info=True)
                    
        except Exception as e:
            self.logger.error(f"Tickæ•°æ®åˆ†ç»„å†™å…¥å¤±è´¥: {e}", exc_info=True)
    
    def save_klines(self, df: pd.DataFrame) -> None:
        """
        æ‰¹é‡ä¿å­˜Kçº¿æ•°æ®åˆ°SQLiteï¼ˆå¼‚æ­¥ï¼ŒåŠ å…¥å†™å…¥é˜Ÿåˆ—ï¼‰
        
        Args:
            df: Kçº¿æ•°æ®DataFrame
            
        Note:
            ä½¿ç”¨æ— é™å¤§å°é˜Ÿåˆ—ï¼ŒåŠ¨æ€æ‰©å®¹
        """
        if df.empty:
            return
        
        try:
            # æ£€æŸ¥é˜Ÿåˆ—å¥åº·çŠ¶æ€
            self._check_queue_health()
            
            # å°†å†™å…¥ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—ï¼ˆæ— é™å¤§å°é˜Ÿåˆ—ï¼Œä¸ä¼šé˜»å¡ï¼‰
            self._write_queue.put(("kline", df.copy()), block=False)
            
        except Exception as e:
            self.logger.error(
                f"åŠ å…¥Kçº¿å†™å…¥é˜Ÿåˆ—å¤±è´¥: {e}ï¼Œ"
                f"å½“å‰é˜Ÿåˆ—å¤§å°: {self._write_queue.qsize()}",
                exc_info=True
            )
    
    def _do_write_klines(self, df: pd.DataFrame) -> None:
        """
        å®é™…æ‰§è¡ŒKçº¿æ•°æ®å†™å…¥ï¼ˆåœ¨å†™å…¥çº¿ç¨‹ä¸­è°ƒç”¨ï¼ŒæŒ‰åˆçº¦+äº¤æ˜“æ—¥åˆ†åº“ï¼‰
        
        Args:
            df: Kçº¿æ•°æ®DataFrameï¼ˆåŒ…å«13ä¸ªæ ¸å¿ƒå­—æ®µï¼ŒPascalCaseå‘½åï¼‰
        """
        try:
            # æ•°æ®æ¸…æ´—ï¼šç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨ï¼ˆPascalCaseå‘½åï¼‰
            required_cols = ["InstrumentID", "ExchangeID", "BarType", "Timestamp", "TradingDay"]
            if not all(col in df.columns for col in required_cols):
                self.logger.warning(f"Kçº¿æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {required_cols}ï¼Œå®é™…å­—æ®µ: {df.columns.tolist()}")
                return
            
            # æŒ‰åˆçº¦å’Œäº¤æ˜“æ—¥åˆ†ç»„
            grouped = df.groupby(["InstrumentID", "TradingDay"])
            
            for (instrument_id, trading_day), group_df in grouped:
                # è·å–è¯¥åˆçº¦+äº¤æ˜“æ—¥çš„æ•°æ®åº“æ–‡ä»¶è·¯å¾„
                db_path = self._get_db_path("kline", str(instrument_id), str(trading_day))
                
                try:
                    conn = sqlite3.connect(str(db_path), timeout=30.0, check_same_thread=False)
                    try:
                        # åˆå§‹åŒ–è¡¨ï¼ˆå¦‚æœæ˜¯æ–°æ•°æ®åº“ï¼‰
                        self._init_kline_table()
                        
                        # å†™å…¥æ•°æ®
                        group_df.to_sql('klines', conn, if_exists='append', index=False)
                        conn.commit()
                        
                        self.logger.debug(
                            f"âœ“ Kçº¿æ•°æ®å†™å…¥æˆåŠŸ: {instrument_id} @ {trading_day} "
                            f"({len(group_df)}æ¡) -> {db_path.name}"
                        )
                    except Exception as e:
                        conn.rollback()
                        self.logger.error(f"å†™å…¥Kçº¿æ•°æ®å¤±è´¥ [{instrument_id}@{trading_day}]: {e}", exc_info=True)
                    finally:
                        conn.close()
                        
                except Exception as e:
                    self.logger.error(f"è¿æ¥Kçº¿æ•°æ®åº“å¤±è´¥ [{instrument_id}@{trading_day}]: {e}", exc_info=True)
                    
        except Exception as e:
            self.logger.error(f"Kçº¿æ•°æ®åˆ†ç»„å†™å…¥å¤±è´¥: {e}", exc_info=True)
    
    def query_ticks(self,
                    instrument_id: str,
                    start_time: str,
                    end_time: str) -> pd.DataFrame:
        """
        æŸ¥è¯¢Tickæ•°æ®ï¼ˆæ”¯æŒåˆ†åº“æŸ¥è¯¢ï¼ŒæŒ‰äº¤æ˜“æ—¥+åˆçº¦å­˜å‚¨ï¼‰
        
        Args:
            instrument_id: åˆçº¦ä»£ç 
            start_time: å¼€å§‹æ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆISOæ ¼å¼ï¼‰
        
        Returns:
            Tickæ•°æ®DataFrameï¼ˆ45ä¸ªå­—æ®µï¼ŒPascalCaseå‘½åï¼‰
        
        Note:
            ä½¿ç”¨ Timestamp å­—æ®µè¿›è¡Œç²¾ç¡®æ—¶é—´èŒƒå›´è¿‡æ»¤ï¼Œ
            è‡ªåŠ¨éå†å¤šä¸ªäº¤æ˜“æ—¥çš„æ•°æ®åº“æ–‡ä»¶
        """
        all_results = []
        
        try:
            # 1. æšä¸¾æ¶‰åŠçš„äº¤æ˜“æ—¥ç›®å½•
            trading_days = self._get_trading_days_between("tick", start_time, end_time)
            
            if not trading_days:
                self.logger.debug(f"æœªæ‰¾åˆ° {instrument_id} åœ¨ {start_time} ~ {end_time} çš„äº¤æ˜“æ—¥æ•°æ®")
                return pd.DataFrame()
            
            # 2. éå†æ¯ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®åº“æ–‡ä»¶
            for trading_day in trading_days:
                db_path = self._get_db_path("tick", instrument_id, trading_day)
                
                if not db_path.exists():
                    self.logger.debug(f"Tickæ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
                    continue
                
                try:
                    # 3. æŸ¥è¯¢è¯¥æ•°æ®åº“
                    with self._get_conn(db_path) as conn:
                        query = """
                            SELECT * FROM ticks
                            WHERE InstrumentID = ?
                            AND Timestamp >= ?
                            AND Timestamp <= ?
                            ORDER BY Timestamp
                        """
                        df = pd.read_sql_query(
                            query, 
                            conn, 
                            params=(instrument_id, start_time, end_time)
                        )
                        
                        if not df.empty:
                            all_results.append(df)
                            self.logger.debug(
                                f"âœ“ ä» {trading_day}/{instrument_id}.db æŸ¥è¯¢åˆ° {len(df)} æ¡Tick"
                            )
                
                except Exception as e:
                    self.logger.warning(f"æŸ¥è¯¢Tickæ•°æ®åº“å¤±è´¥ [{trading_day}/{instrument_id}.db]: {e}")
                    continue
            
            # 4. åˆå¹¶æ‰€æœ‰ç»“æœ
            if all_results:
                merged_df = pd.concat(all_results, ignore_index=True)
                # æŒ‰æ—¶é—´æ’åºï¼ˆç¡®ä¿è·¨äº¤æ˜“æ—¥æ•°æ®æœ‰åºï¼‰
                merged_df = merged_df.sort_values('Timestamp').reset_index(drop=True)
                self.logger.debug(f"æŸ¥è¯¢åˆ° {len(merged_df)} æ¡Tickæ•°æ®ï¼ˆåˆå¹¶è‡ª {len(all_results)} ä¸ªäº¤æ˜“æ—¥ï¼‰")
                return merged_df
            
            self.logger.debug(f"æœªæŸ¥è¯¢åˆ°Tickæ•°æ®: {instrument_id} {start_time} ~ {end_time}")
            return pd.DataFrame()
        
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢Tickæ•°æ®å¤±è´¥: {e}", exc_info=True)
            return pd.DataFrame()
    
    def query_klines(self,
                     instrument_id: str,
                     interval: str,
                     start_time: str,
                     end_time: str) -> pd.DataFrame:
        """
        æŸ¥è¯¢Kçº¿æ•°æ®ï¼ˆæ”¯æŒåˆ†åº“æŸ¥è¯¢ï¼ŒæŒ‰äº¤æ˜“æ—¥+åˆçº¦å­˜å‚¨ï¼‰
        
        Args:
            instrument_id: åˆçº¦ä»£ç 
            interval: Kçº¿å‘¨æœŸï¼ˆå¯¹åº”BarTypeå­—æ®µï¼‰
            start_time: å¼€å§‹æ—¶é—´ï¼ˆISOæ ¼å¼ï¼Œå¦‚ '2025-10-27 14:00:00'ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆISOæ ¼å¼ï¼Œå¦‚ '2025-10-28 16:00:00'ï¼‰
        
        Returns:
            Kçº¿æ•°æ®DataFrameï¼ˆ13ä¸ªå­—æ®µï¼ŒPascalCaseå‘½åï¼‰
        
        Note:
            ä½¿ç”¨ Timestamp å­—æ®µï¼ˆå®Œæ•´datetimeï¼‰è¿›è¡Œæ—¶é—´èŒƒå›´è¿‡æ»¤ï¼Œ
            æ”¯æŒç²¾ç¡®çš„è·¨å¤©æŸ¥è¯¢ï¼Œè‡ªåŠ¨éå†å¤šä¸ªäº¤æ˜“æ—¥çš„æ•°æ®åº“æ–‡ä»¶
        """
        all_results = []
        
        try:
            # 1. æšä¸¾æ¶‰åŠçš„äº¤æ˜“æ—¥ç›®å½•
            trading_days = self._get_trading_days_between("kline", start_time, end_time)
            
            if not trading_days:
                self.logger.debug(f"æœªæ‰¾åˆ° {instrument_id} åœ¨ {start_time} ~ {end_time} çš„äº¤æ˜“æ—¥æ•°æ®")
                return pd.DataFrame()
            
            # 2. éå†æ¯ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®åº“æ–‡ä»¶
            for trading_day in trading_days:
                db_path = self._get_db_path("kline", instrument_id, trading_day)
                
                if not db_path.exists():
                    self.logger.debug(f"Kçº¿æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
                    continue
                
                try:
                    # 3. æŸ¥è¯¢è¯¥æ•°æ®åº“
                    with self._get_conn(db_path) as conn:
                        query = """
                            SELECT * FROM klines
                            WHERE InstrumentID = ?
                            AND BarType = ?
                            AND Timestamp >= ?
                            AND Timestamp <= ?
                            ORDER BY Timestamp
                        """
                        df = pd.read_sql_query(
                            query, 
                            conn, 
                            params=(instrument_id, interval, start_time, end_time)
                        )
                        
                        if not df.empty:
                            all_results.append(df)
                            self.logger.debug(
                                f"âœ“ ä» {trading_day}/{instrument_id}.db æŸ¥è¯¢åˆ° {len(df)} æ¡Kçº¿"
                            )
                
                except Exception as e:
                    self.logger.warning(f"æŸ¥è¯¢Kçº¿æ•°æ®åº“å¤±è´¥ [{trading_day}/{instrument_id}.db]: {e}")
                    continue
            
            # 4. åˆå¹¶æ‰€æœ‰ç»“æœ
            if all_results:
                merged_df = pd.concat(all_results, ignore_index=True)
                # æŒ‰æ—¶é—´æ’åºï¼ˆç¡®ä¿è·¨äº¤æ˜“æ—¥æ•°æ®æœ‰åºï¼‰
                merged_df = merged_df.sort_values('Timestamp').reset_index(drop=True)
                self.logger.debug(f"æŸ¥è¯¢åˆ° {len(merged_df)} æ¡Kçº¿æ•°æ®ï¼ˆåˆå¹¶è‡ª {len(all_results)} ä¸ªäº¤æ˜“æ—¥ï¼‰")
                return merged_df
            
            self.logger.debug(f"æœªæŸ¥è¯¢åˆ°Kçº¿æ•°æ®: {instrument_id} ({interval}) {start_time} ~ {end_time}")
            return pd.DataFrame()
        
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢Kçº¿æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return pd.DataFrame()
    
    def get_archivable_data(self, cutoff_date: str) -> dict:
        """
        è·å–å¯å½’æ¡£çš„æ•°æ®ï¼ˆè¶…è¿‡ä¿ç•™æœŸçš„æ•°æ®ï¼‰
        
        Args:
            cutoff_date: æˆªæ­¢æ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD
        
        Returns:
            {"ticks": DataFrame, "klines": DataFrame}
        """
        result = {}
        
        try:
            # æŸ¥è¯¢Tickæ•°æ®
            with self._get_conn(self.tick_db_file) as conn:
                query = "SELECT * FROM ticks WHERE datetime < ?"
                result['ticks'] = pd.read_sql_query(query, conn, params=(cutoff_date,))
            
            # æŸ¥è¯¢Kçº¿æ•°æ®
            with self._get_conn(self.kline_db_file) as conn:
                query = "SELECT * FROM klines WHERE datetime < ?"
                result['klines'] = pd.read_sql_query(query, conn, params=(cutoff_date,))
            
            self.logger.info(
                f"è·å–å¯å½’æ¡£æ•°æ®: Tick={len(result['ticks'])}æ¡, Kçº¿={len(result['klines'])}æ¡"
            )
            return result
        
        except Exception as e:
            self.logger.error(f"è·å–å¯å½’æ¡£æ•°æ®å¤±è´¥: {e}", exc_info=True)
            return {"ticks": pd.DataFrame(), "klines": pd.DataFrame()}
    
    def delete_archived_data(self, cutoff_date: str) -> None:
        """
        åˆ é™¤å·²å½’æ¡£çš„æ•°æ®
        
        Args:
            cutoff_date: æˆªæ­¢æ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DD
        """
        try:
            # åˆ é™¤Tickæ•°æ®
            with self._get_conn(self.tick_db_file) as conn:
                cursor = conn.execute("DELETE FROM ticks WHERE datetime < ?", (cutoff_date,))
                tick_count = cursor.rowcount
                self.logger.info(f"åˆ é™¤ {tick_count} æ¡å·²å½’æ¡£Tickæ•°æ®")
            
            # åˆ é™¤Kçº¿æ•°æ®
            with self._get_conn(self.kline_db_file) as conn:
                cursor = conn.execute("DELETE FROM klines WHERE datetime < ?", (cutoff_date,))
                kline_count = cursor.rowcount
                self.logger.info(f"åˆ é™¤ {kline_count} æ¡å·²å½’æ¡£Kçº¿æ•°æ®")
            
            # æ‰§è¡ŒVACUUMä»¥å›æ”¶ç©ºé—´
            self._vacuum_databases()
        
        except Exception as e:
            self.logger.error(f"åˆ é™¤å·²å½’æ¡£æ•°æ®å¤±è´¥: {e}", exc_info=True)
    
    def _vacuum_databases(self) -> None:
        """å‹ç¼©æ•°æ®åº“ï¼Œå›æ”¶ç©ºé—´"""
        try:
            with self._get_conn(self.tick_db_file) as conn:
                conn.execute("VACUUM")
            
            with self._get_conn(self.kline_db_file) as conn:
                conn.execute("VACUUM")
            
            self.logger.info("æ•°æ®åº“å‹ç¼©å®Œæˆ")
        
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“å‹ç¼©å¤±è´¥: {e}", exc_info=True)
    
    def get_statistics(self) -> dict:
        """
        è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            "retention_days": self.retention_days,
            "tick_count": 0,
            "kline_count": 0,
            "db_size_mb": 0.0
        }
        
        try:
            # ç»Ÿè®¡Tickæ•°é‡
            with self._get_conn(self.tick_db_file) as conn:
                cursor = conn.execute("SELECT COUNT(*) FROM ticks")
                stats["tick_count"] = cursor.fetchone()[0]
            
            # ç»Ÿè®¡Kçº¿æ•°é‡
            with self._get_conn(self.kline_db_file) as conn:
                cursor = conn.execute("SELECT COUNT(*) FROM klines")
                stats["kline_count"] = cursor.fetchone()[0]
            
            # ç»Ÿè®¡æ•°æ®åº“æ–‡ä»¶å¤§å°
            if self.tick_db_file.exists():
                stats["db_size_mb"] += self.tick_db_file.stat().st_size / (1024 * 1024)
            if self.kline_db_file.exists():
                stats["db_size_mb"] += self.kline_db_file.stat().st_size / (1024 * 1024)
            
            stats["db_size_mb"] = round(stats["db_size_mb"], 2)
        
        except Exception as e:
            self.logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
        
        return stats

