#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@ProjectName: homalos-datacenter
@FileName   : duckdb_storage.py
@Date       : 2025/10/29
@Author     : Lumosylva
@Email      : donnymoving@gmail.com
@Software   : PyCharm
@Description: DuckDBå­˜å‚¨ - æŒ‰äº¤æ˜“æ—¥åˆ†æ–‡ä»¶ + æŒ‰åˆçº¦åˆ†è¡¨ï¼Œæé€ŸæŸ¥è¯¢å¼•æ“
"""
import re
import duckdb
import pandas as pd
import threading
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime, timedelta
from collections import defaultdict

from src.system_config import Config
from src.utils.log import get_logger
from src.core.trading_day_manager import TradingDayManager


def normalize_instrument_id(instrument_id: str) -> str:
    """
    è§„èŒƒåŒ–åˆçº¦IDä¸ºåˆæ³•çš„SQLè¡¨å
    
    è§„åˆ™ï¼š
    - è½¬å°å†™
    - ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼‰
    - ç¡®ä¿ä»¥å­—æ¯å¼€å¤´ï¼ˆSQLè¡¨åè¦æ±‚ï¼‰
    
    Args:
        instrument_id: åŸå§‹åˆçº¦IDï¼ˆå¦‚ sa601, rb2511, IF2501ï¼‰
    
    Returns:
        è§„èŒƒåŒ–åçš„è¡¨åï¼ˆå¦‚ sa601, rb2511, if2501ï¼‰
    
    Examples:
        >>> normalize_instrument_id('sa601')
        'sa601'
        >>> normalize_instrument_id('IF2501')
        'if2501'
        >>> normalize_instrument_id('IC-2501')
        'ic2501'
    """
    if not instrument_id:
        return 'unknown'
    
    # è½¬å°å†™
    normalized = instrument_id.lower()
    
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼‰
    normalized = re.sub(r'[^a-z0-9_]', '', normalized)
    
    # ç¡®ä¿ä»¥å­—æ¯å¼€å¤´ï¼ˆSQLè¡¨åè¦æ±‚ï¼‰
    if normalized and normalized[0].isdigit():
        normalized = f"c{normalized}"
    
    return normalized or 'unknown'


def create_tick_table_sql(instrument_id: str) -> str:
    """
    ç”Ÿæˆåˆ›å»ºTickè¡¨çš„SQLï¼ˆæŒ‰åˆçº¦åˆ†è¡¨ï¼‰
    
    Args:
        instrument_id: åˆçº¦IDï¼ˆå¦‚ sa601ï¼‰
    
    Returns:
        CREATE TABLE SQLè¯­å¥
    """
    table_name = f"tick_{normalize_instrument_id(instrument_id)}"
    
    return f"""
    CREATE TABLE IF NOT EXISTS {table_name} (
        TradingDay DATE,
        ExchangeID VARCHAR,
        LastPrice DOUBLE,
        PreSettlementPrice DOUBLE,
        PreClosePrice DOUBLE,
        PreOpenInterest BIGINT,
        OpenPrice DOUBLE,
        HighestPrice DOUBLE,
        LowestPrice DOUBLE,
        Volume BIGINT,
        Turnover DOUBLE,
        OpenInterest BIGINT,
        ClosePrice DOUBLE,
        SettlementPrice DOUBLE,
        UpperLimitPrice DOUBLE,
        LowerLimitPrice DOUBLE,
        PreDelta DOUBLE,
        CurrDelta DOUBLE,
        UpdateTime VARCHAR,
        UpdateMillisec INTEGER,
        BidPrice1 DOUBLE,
        BidVolume1 BIGINT,
        AskPrice1 DOUBLE,
        AskVolume1 BIGINT,
        BidPrice2 DOUBLE,
        BidVolume2 BIGINT,
        AskPrice2 DOUBLE,
        AskVolume2 BIGINT,
        BidPrice3 DOUBLE,
        BidVolume3 BIGINT,
        AskPrice3 DOUBLE,
        AskVolume3 BIGINT,
        BidPrice4 DOUBLE,
        BidVolume4 BIGINT,
        AskPrice4 DOUBLE,
        AskVolume4 BIGINT,
        BidPrice5 DOUBLE,
        BidVolume5 BIGINT,
        AskPrice5 DOUBLE,
        AskVolume5 BIGINT,
        AveragePrice DOUBLE,
        ActionDay VARCHAR,
        InstrumentID VARCHAR,
        ExchangeInstID VARCHAR,
        BandingUpperPrice DOUBLE,
        BandingLowerPrice DOUBLE,
        Timestamp TIMESTAMP
    )
    """


def create_kline_table_sql(instrument_id: str) -> str:
    """
    ç”Ÿæˆåˆ›å»ºKçº¿è¡¨çš„SQLï¼ˆæŒ‰åˆçº¦åˆ†è¡¨ï¼‰
    
    Args:
        instrument_id: åˆçº¦IDï¼ˆå¦‚ sa601ï¼‰
    
    Returns:
        CREATE TABLE SQLè¯­å¥
    
    Note:
        å­—æ®µå®šä¹‰ä¸ bar_manager.py çš„ _bar_to_dataframe å®Œå…¨ä¸€è‡´ï¼ˆ13ä¸ªå­—æ®µï¼‰
    """
    table_name = f"kline_{normalize_instrument_id(instrument_id)}"
    
    return f"""
    CREATE TABLE IF NOT EXISTS {table_name} (
        BarType VARCHAR,
        TradingDay VARCHAR,
        UpdateTime VARCHAR,
        InstrumentID VARCHAR,
        ExchangeID VARCHAR,
        Volume BIGINT,
        OpenInterest BIGINT,
        OpenPrice DOUBLE,
        HighestPrice DOUBLE,
        LowestPrice DOUBLE,
        ClosePrice DOUBLE,
        LastVolume BIGINT,
        Timestamp TIMESTAMP
    )
    """


class DuckDBSingleFileWriter:
    """
    DuckDBå•æ–‡ä»¶å†™å…¥å™¨ - æŒ‰äº¤æ˜“æ—¥åˆ†æ–‡ä»¶ï¼Œæ–‡ä»¶å†…æ’åºèšç±»
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. æŒ‰æ—¥åˆ†æ–‡ä»¶ï¼šæ¯ä¸ªäº¤æ˜“æ—¥ä¸€ä¸ª.duckdbæ–‡ä»¶
    2. æ’åºæ’å…¥ï¼šä¿è¯åŒä¸€åˆçº¦æ•°æ®ç‰©ç†è¿ç»­ï¼ˆè§¦å‘Zone Mapsä¼˜åŒ–ï¼‰
    3. æ‰¹é‡å†™å…¥ï¼šç´¯ç§¯åˆ°é˜ˆå€¼åæ‰¹é‡INSERT
    4. å•çº¿ç¨‹å†™å…¥ï¼šæ— å¹¶å‘ç«äº‰ï¼Œç®€åŒ–é€»è¾‘
    
    æ€§èƒ½ä¼˜åŠ¿ï¼š
    - å•æ—¥æŸ¥è¯¢ï¼šç›´æ¥è¯»å•æ–‡ä»¶ï¼ˆ~12msï¼‰
    - è·¨æ—¥æŸ¥è¯¢ï¼šATTACHå¤šæ–‡ä»¶å¹¶è¡Œï¼ˆ~80msï¼‰
    - å‹ç¼©ç‡ï¼š77%ï¼ˆåŸå§‹æ•°æ®å‹ç¼©åˆ°23%ï¼‰
    """
    
    def __init__(self,
                 db_path: str = "data/duckdb/ticks",
                 batch_threshold: int = 10000,
                 data_type: str = "ticks",
                 trading_day_manager: Optional[TradingDayManager] = None):
        """
        åˆå§‹åŒ–DuckDBå†™å…¥å™¨
        
        Args:
            db_path: DuckDBæ–‡ä»¶æ ¹ç›®å½•
            batch_threshold: æ‰¹é‡å†™å…¥é˜ˆå€¼ï¼ˆç´¯ç§¯å¤šå°‘æ¡è§¦å‘å†™å…¥ï¼‰
            data_type: æ•°æ®ç±»å‹ï¼ˆ"ticks"æˆ–"klines"ï¼‰
            trading_day_manager: äº¤æ˜“æ—¥ç®¡ç†å™¨
        """
        self.db_path = Path(db_path)
        self.db_path.mkdir(parents=True, exist_ok=True)
        self.batch_threshold = batch_threshold
        self.data_type = data_type
        self.trading_day_manager = trading_day_manager
        self.logger = get_logger(self.__class__.__name__)
        
        # å•æ—¥ç¼“å†²åŒº: {trading_day: [df1, df2, ...]}
        self.daily_buffer: Dict[str, List[pd.DataFrame]] = defaultdict(list)
        self.buffer_lock = threading.Lock()
        
        # æ–‡ä»¶é”ï¼šé˜²æ­¢å¤šä¸ªçº¿ç¨‹åŒæ—¶å†™å…¥åŒä¸€ä¸ªDuckDBæ–‡ä»¶
        self.file_locks: Dict[str, threading.Lock] = {}
        self.locks_lock = threading.Lock()
        
        # çº¿ç¨‹è·Ÿè¸ªï¼šç›‘æ§å’Œæ¸…ç†åƒµå°¸çº¿ç¨‹ï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°ï¼‰
        self.active_threads: Dict[str, Dict] = {}  # {thread_name: {start_time, trading_day, row_count}}
        self.thread_track_lock = threading.Lock()
        self.max_thread_lifetime = Config.duckdb_max_thread_lifetime  # ä»é…ç½®è¯»å–
        self.submit_count = 0  # æäº¤è®¡æ•°å™¨ï¼Œç”¨äºå®šæœŸè§¦å‘ç›‘æ§
        self.monitor_interval = Config.duckdb_monitor_interval  # ä»é…ç½®è¯»å–
        
        # åˆ†è¡¨æ¶æ„ï¼šä¸å†éœ€è¦å•ä¸€å»ºè¡¨SQLï¼Œåœ¨å†™å…¥æ—¶åŠ¨æ€ç”Ÿæˆ
        
        self.logger.info(
            f"DuckDBå†™å…¥å™¨å·²åˆå§‹åŒ–ï¼ˆæŒ‰åˆçº¦åˆ†è¡¨ + æ–‡ä»¶é”ä¿æŠ¤ï¼‰ï¼šè·¯å¾„={db_path}ï¼Œ"
            f"æ‰¹é‡é˜ˆå€¼={batch_threshold}ï¼Œç±»å‹={data_type}"
        )
    
    def submit_batch(self, df: pd.DataFrame) -> None:
        """
        æäº¤ä¸€æ‰¹æ•°æ®ï¼ˆè‡ªåŠ¨æŒ‰äº¤æ˜“æ—¥åˆ†ç»„ï¼‰
        
        æ”¹è¿›ï¼šåœ¨é”å†…æå–æ•°æ®ï¼Œåå°çº¿ç¨‹å¼‚æ­¥åˆ·æ–°ï¼ˆé¿å…æŒé”é˜»å¡ï¼‰
        
        Args:
            df: æ•°æ®DataFrameï¼ˆå¿…é¡»åŒ…å«TradingDayå’ŒInstrumentIDåˆ—ï¼‰
        
        Raises:
            ValueError: å¦‚æœdfç¼ºå°‘å¿…è¦åˆ—
        """
        if df.empty:
            self.logger.warning("æäº¤çš„DataFrameä¸ºç©ºï¼Œå·²è·³è¿‡")
            return
        
        # éªŒè¯å¿…è¦åˆ—
        required_columns = ['TradingDay', 'InstrumentID']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"DataFrameç¼ºå°‘å¿…è¦åˆ—ï¼š{missing_columns}")
        
        # å®šæœŸç›‘æ§çº¿ç¨‹ï¼ˆæ¯10æ¬¡æäº¤æ£€æŸ¥ä¸€æ¬¡ï¼‰
        self.submit_count += 1
        if self.submit_count % self.monitor_interval == 0:
            try:
                stats = self._monitor_and_cleanup_threads()
                if stats['zombie_threads'] > 0 or stats['flush_threads'] > 20:
                    self.logger.warning(
                        f"çº¿ç¨‹ç›‘æ§ï¼šæ€»çº¿ç¨‹={stats['total_threads']}ï¼Œ"
                        f"åˆ·æ–°çº¿ç¨‹={stats['flush_threads']}ï¼Œ"
                        f"åƒµå°¸çº¿ç¨‹={stats['zombie_threads']}ï¼Œ"
                        f"è·Ÿè¸ªçº¿ç¨‹={stats['active_tracked']}ï¼Œ"
                        f"å·²æ¸…ç†={stats['cleaned']}"
                    )
            except Exception as e:
                self.logger.error(f"çº¿ç¨‹ç›‘æ§å¤±è´¥ï¼š{e}")
        
        # åœ¨é”å†…è¿½åŠ æ•°æ®å¹¶åˆ¤æ–­æ˜¯å¦åˆ·æ–°
        with self.buffer_lock:
            for trading_day, group_df in df.groupby('TradingDay'):
                # è½¬æ¢æ—¥æœŸæ ¼å¼ï¼ˆæ”¯æŒYYYY-MM-DDæˆ–YYYYMMDDï¼‰
                day_key = str(trading_day).replace('-', '')[:8]
                
                # æ·»åŠ åˆ°ç¼“å†²åŒº
                self.daily_buffer[day_key].append(group_df)
                
                # è®¡ç®—è¯¥æ—¥ç¼“å†²åŒºæ€»è¡Œæ•°
                total_rows = sum(len(d) for d in self.daily_buffer[day_key])
                
                # è¾¾åˆ°é˜ˆå€¼æ—¶åˆ·æ–°
                if total_rows >= self.batch_threshold:
                    # æ£€æŸ¥å½“å‰DuckDBåˆ·æ–°çº¿ç¨‹æ•°é‡ï¼ˆé˜²æ­¢çº¿ç¨‹æ³„æ¼ï¼‰
                    flush_threads = [
                        t for t in threading.enumerate() 
                        if t.name.startswith("DuckDB-Flush-")
                    ]
                    if len(flush_threads) > 10:
                        self.logger.warning(
                            f"DuckDBåˆ·æ–°çº¿ç¨‹æ•°é‡è¿‡å¤šï¼š{len(flush_threads)}ä¸ªï¼Œ"
                            f"å¯èƒ½å­˜åœ¨çº¿ç¨‹é˜»å¡æˆ–æ³„æ¼"
                        )
                    
                    # å…³é”®æ”¹è¿›ï¼šåœ¨é”å†…popæ•°æ®ï¼Œç„¶åå¯åŠ¨åå°çº¿ç¨‹å¼‚æ­¥åˆ·æ–°
                    dfs_to_flush = self.daily_buffer.pop(day_key)
                    
                    # å¯åŠ¨åå°çº¿ç¨‹ï¼ˆåœ¨é”å†…ï¼Œä½†Thread.start()å¾ˆå¿«<1msï¼‰
                    threading.Thread(
                        target=self._flush_day_async,
                        args=(day_key, dfs_to_flush),
                        name=f"DuckDB-Flush-{day_key}",
                        daemon=True
                    ).start()
                    
                    self.logger.info(
                        f"DuckDBè¾¾åˆ°æ‰¹é‡é˜ˆå€¼ï¼Œå¯åŠ¨åå°çº¿ç¨‹åˆ·æ–°ï¼š{day_key}ï¼Œ{total_rows}æ¡ "
                        f"(å½“å‰æ´»åŠ¨çº¿ç¨‹: {len(flush_threads)+1})"
                    )
    
    def _get_file_lock(self, trading_day: str) -> threading.Lock:
        """
        è·å–æŒ‡å®šäº¤æ˜“æ—¥çš„æ–‡ä»¶é”ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        
        Args:
            trading_day: äº¤æ˜“æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰
        
        Returns:
            è¯¥äº¤æ˜“æ—¥å¯¹åº”çš„æ–‡ä»¶é”
        
        Note:
            ä½¿ç”¨locks_lockä¿æŠ¤file_lockså­—å…¸çš„å¹¶å‘è®¿é—®
        """
        with self.locks_lock:
            if trading_day not in self.file_locks:
                self.file_locks[trading_day] = threading.Lock()
            return self.file_locks[trading_day]
    
    def _monitor_and_cleanup_threads(self) -> Dict:
        """
        ç›‘æ§å¹¶æ¸…ç†åƒµå°¸çº¿ç¨‹
        
        Returns:
            {
                'total_threads': int,  # æ€»çº¿ç¨‹æ•°
                'flush_threads': int,  # DuckDBåˆ·æ–°çº¿ç¨‹æ•°
                'zombie_threads': int,  # åƒµå°¸çº¿ç¨‹æ•°
                'cleaned': int  # å·²æ¸…ç†çš„çº¿ç¨‹æ•°
            }
        """
        import time
        current_time = time.time()
        
        # è·å–æ‰€æœ‰DuckDBåˆ·æ–°çº¿ç¨‹
        all_threads = threading.enumerate()
        flush_threads = [t for t in all_threads if t.name.startswith("DuckDB-Flush-")]
        
        zombie_threads = []
        cleaned_count = 0
        
        # æ£€æŸ¥è·Ÿè¸ªçš„çº¿ç¨‹
        with self.thread_track_lock:
            for thread_name, info in list(self.active_threads.items()):
                thread_age = current_time - info['start_time']
                
                # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦è¿˜å­˜æ´»
                thread_alive = any(t.name == thread_name for t in all_threads)
                
                if not thread_alive:
                    # çº¿ç¨‹å·²å®Œæˆï¼Œä»è·Ÿè¸ªä¸­ç§»é™¤
                    del self.active_threads[thread_name]
                    cleaned_count += 1
                elif thread_age > self.max_thread_lifetime:
                    # è¶…æ—¶çº¿ç¨‹ï¼Œè§†ä¸ºåƒµå°¸çº¿ç¨‹
                    zombie_threads.append({
                        'name': thread_name,
                        'age': thread_age,
                        'trading_day': info['trading_day'],
                        'row_count': info['row_count']
                    })
                    self.logger.error(
                        f"ğŸ§Ÿ æ£€æµ‹åˆ°åƒµå°¸çº¿ç¨‹ï¼š{thread_name}ï¼Œ"
                        f"å·²è¿è¡Œ{thread_age:.1f}ç§’ï¼ˆè¶…æ—¶é˜ˆå€¼{self.max_thread_lifetime}ç§’ï¼‰ï¼Œ"
                        f"äº¤æ˜“æ—¥={info['trading_day']}ï¼Œæ•°æ®é‡={info['row_count']}æ¡"
                    )
        
        # è®°å½•è­¦å‘Š
        if zombie_threads:
            self.logger.warning(
                f"å‘ç°{len(zombie_threads)}ä¸ªåƒµå°¸çº¿ç¨‹ï¼Œ"
                f"æ€»åˆ·æ–°çº¿ç¨‹æ•°={len(flush_threads)}"
            )
        
        return {
            'total_threads': len(all_threads),
            'flush_threads': len(flush_threads),
            'zombie_threads': len(zombie_threads),
            'cleaned': cleaned_count,
            'active_tracked': len(self.active_threads)
        }
    
    def _flush_day_async(self, trading_day: str, dfs: List[pd.DataFrame]) -> None:
        """
        å¼‚æ­¥åˆ·æ–°å•æ—¥æ•°æ®åˆ°DuckDBæ–‡ä»¶ï¼ˆåœ¨åå°çº¿ç¨‹æ‰§è¡Œï¼‰
        
        Args:
            trading_day: äº¤æ˜“æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰
            dfs: å¾…åˆ·æ–°çš„DataFrameåˆ—è¡¨
        
        å…³é”®ï¼šæ­¤æ–¹æ³•åœ¨åå°çº¿ç¨‹æ‰§è¡Œï¼Œä¸æŒbuffer_lockï¼Œä¸é˜»å¡æ–°æ•°æ®è¿½åŠ 
        
        å®ç°è¦ç‚¹ï¼š
        1. åˆå¹¶è¯¥æ—¥çš„æ‰€æœ‰æ‰¹æ¬¡æ•°æ®
        2. æŒ‰InstrumentID, Timestampæ’åºï¼ˆä¿è¯æ—¶é—´åºåˆ—è¿ç»­æ€§ï¼‰
        3. åˆ›å»ºæˆ–æ‰“å¼€å¯¹åº”çš„.duckdbæ–‡ä»¶
        4. æŒ‰åˆçº¦åˆ†ç»„ï¼Œä¸ºæ¯ä¸ªåˆçº¦åˆ›å»ºç‹¬ç«‹çš„è¡¨
        5. æ¯ä¸ªåˆçº¦çš„æ•°æ®å†™å…¥å¯¹åº”çš„è¡¨ï¼ˆå¤©ç„¶ç‰©ç†è¿ç»­ï¼ï¼‰
        """
        if not dfs:
            return
        
        # è®°å½•çº¿ç¨‹å¼€å§‹
        import time
        thread_name = threading.current_thread().name
        start_time = time.time()
        merged_df = pd.concat(dfs, ignore_index=True)
        row_count = len(merged_df)
        
        # æ³¨å†Œåˆ°çº¿ç¨‹è·Ÿè¸ª
        with self.thread_track_lock:
            self.active_threads[thread_name] = {
                'start_time': start_time,
                'trading_day': trading_day,
                'row_count': row_count
            }
        
        # 2. æ’åºï¼ˆä¿è¯æ—¶é—´åºåˆ—è¿ç»­æ€§ï¼‰
        merged_df = merged_df.sort_values(
            by=['InstrumentID', 'Timestamp']
        ).reset_index(drop=True)
        
        # 3. è·å–æ–‡ä»¶é”ï¼ˆé˜²æ­¢å¹¶å‘å†™å…¥åŒä¸€ä¸ªDuckDBæ–‡ä»¶ï¼‰
        file_lock = self._get_file_lock(trading_day)
        db_file = self.db_path / f"{trading_day}.duckdb"
        
        # ä½¿ç”¨æ–‡ä»¶é”ä¿æŠ¤æ•´ä¸ªå†™å…¥è¿‡ç¨‹
        with file_lock:
            self.logger.debug(f"è·å–æ–‡ä»¶é”æˆåŠŸï¼š{trading_day}ï¼Œå¼€å§‹å†™å…¥...")
            
            # æ‰“å¼€DuckDBè¿æ¥
            conn = duckdb.connect(str(db_file))
            
            try:
                # 4. æŒ‰åˆçº¦åˆ†ç»„å†™å…¥ï¼ˆæ¯ä¸ªåˆçº¦ä¸€å¼ è¡¨ï¼‰
                conn.execute("BEGIN TRANSACTION")
                
                contracts_written = []
                total_rows = 0
                
                # æŒ‰InstrumentIDåˆ†ç»„ï¼ˆå·²æ’åºï¼Œé«˜æ•ˆï¼‰
                for instrument_id, group_df in merged_df.groupby('InstrumentID', sort=False):
                    # 4.1 ç”Ÿæˆè¡¨åå’Œåˆ›å»ºSQL
                    if self.data_type == 'ticks':
                        create_sql = create_tick_table_sql(instrument_id)
                        table_name = f"tick_{normalize_instrument_id(instrument_id)}"
                    else:  # klines
                        create_sql = create_kline_table_sql(instrument_id)
                        table_name = f"kline_{normalize_instrument_id(instrument_id)}"
                    
                    # 4.2 åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                    conn.execute(create_sql)
                    
                    # 4.3 æ³¨å†ŒDataFrameä¸ºä¸´æ—¶è¡¨
                    conn.register('temp_df', group_df)
                    
                    # 4.4 æ‰¹é‡æ’å…¥
                    conn.execute(f"INSERT INTO {table_name} SELECT * FROM temp_df")
                    
                    # 4.5 å–æ¶ˆæ³¨å†Œ
                    conn.unregister('temp_df')
                    
                    contracts_written.append(instrument_id)
                    total_rows += len(group_df)
                
                # 5. æäº¤äº‹åŠ¡
                conn.execute("COMMIT")
                
                self.logger.info(
                    f"âœ“ DuckDBå¼‚æ­¥å†™å…¥æˆåŠŸï¼š{trading_day}ï¼Œ{total_rows}æ¡ï¼Œ"
                    f"{len(contracts_written)}ä¸ªåˆçº¦ | "
                    f"ç¤ºä¾‹(å‰5ä¸ªåˆçº¦)ï¼š{contracts_written[:5]}"
                )
                
            except Exception as e:
                # å›æ»šäº‹åŠ¡
                try:
                    conn.execute("ROLLBACK")
                except Exception:
                    pass
                
                self.logger.error(
                    f"DuckDBå¼‚æ­¥å†™å…¥å¤±è´¥ [{trading_day}]ï¼š{e}",
                    exc_info=True
                )
                raise
            
            finally:
                # 6. å…³é—­è¿æ¥
                conn.close()
                self.logger.debug(f"é‡Šæ”¾æ–‡ä»¶é”ï¼š{trading_day}ï¼Œå†™å…¥å®Œæˆ")
                
                # è®°å½•çº¿ç¨‹ç»“æŸï¼Œä»è·Ÿè¸ªä¸­ç§»é™¤
                end_time = time.time()
                elapsed = end_time - start_time
                with self.thread_track_lock:
                    if thread_name in self.active_threads:
                        del self.active_threads[thread_name]
                
                self.logger.debug(
                    f"çº¿ç¨‹{thread_name}å®Œæˆï¼Œè€—æ—¶{elapsed:.2f}ç§’ï¼Œ"
                    f"æ•°æ®é‡={row_count}æ¡"
                )
    
    def stop(self, timeout: float = 30.0) -> None:
        """
        åœæ­¢å†™å…¥å™¨ï¼Œåˆ·æ–°æ‰€æœ‰å‰©ä½™æ•°æ®
        
        Args:
            timeout: ç­‰å¾…åå°åˆ·æ–°çº¿ç¨‹å®Œæˆçš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.logger.info(f"æ­£åœ¨åœæ­¢DuckDBå†™å…¥å™¨ ({self.data_type})...")
        
        # 1. åˆ·æ–°æ‰€æœ‰å‰©ä½™ç¼“å†²åŒºï¼ˆåŒæ­¥åˆ·æ–°ï¼Œé¿å…å¯åŠ¨æ–°çº¿ç¨‹ï¼‰
        with self.buffer_lock:
            days_to_flush = list(self.daily_buffer.keys())
        
        for day in days_to_flush:
            with self.buffer_lock:
                if day in self.daily_buffer:
                    dfs = self.daily_buffer.pop(day)
                    if dfs:
                        self.logger.info(f"åˆ·æ–°å‰©ä½™æ•°æ®ï¼š{day}ï¼Œ{sum(len(d) for d in dfs)}æ¡")
                        # åŒæ­¥åˆ·æ–°ï¼ˆä¼˜é›…å…³é—­æ—¶ä¸å¯åŠ¨æ–°çº¿ç¨‹ï¼‰
                        self._flush_day_sync(day, dfs)
        
        # 2. ç­‰å¾…æ‰€æœ‰åå°çº¿ç¨‹å®Œæˆ
        import time
        start_wait = time.time()
        
        # ç›‘æ§åå°åˆ·æ–°çº¿ç¨‹
        while time.time() - start_wait < timeout:
            flush_threads = [
                t for t in threading.enumerate() 
                if t.name.startswith("DuckDB-Flush-")
            ]
            if not flush_threads:
                break
            
            # è¯¦ç»†æ—¥å¿—ï¼šæ˜¾ç¤ºçº¿ç¨‹åç§°
            thread_names = [t.name for t in flush_threads[:5]]  # åªæ˜¾ç¤ºå‰5ä¸ª
            self.logger.info(
                f"ç­‰å¾…{len(flush_threads)}ä¸ªåå°åˆ·æ–°çº¿ç¨‹å®Œæˆ... "
                f"ç¤ºä¾‹ï¼š{thread_names}"
            )
            time.sleep(0.5)
        
        # æ£€æŸ¥æ˜¯å¦ä»æœ‰çº¿ç¨‹
        remaining_threads = [
            t for t in threading.enumerate() 
            if t.name.startswith("DuckDB-Flush-")
        ]
        if remaining_threads:
            self.logger.warning(
                f"ä»æœ‰{len(remaining_threads)}ä¸ªåå°çº¿ç¨‹æœªå®Œæˆï¼š{[t.name for t in remaining_threads]}"
            )
        
        # å¼ºåˆ¶æ¸…ç†è·Ÿè¸ªçš„åƒµå°¸çº¿ç¨‹
        with self.thread_track_lock:
            if self.active_threads:
                zombie_count = len(self.active_threads)
                zombie_names = list(self.active_threads.keys())[:5]  # æ˜¾ç¤ºå‰5ä¸ª
                self.logger.warning(
                    f"ğŸ§Ÿ å¼ºåˆ¶æ¸…ç†{zombie_count}ä¸ªåƒµå°¸çº¿ç¨‹ï¼š{zombie_names}"
                )
                self.active_threads.clear()
        
        self.logger.info(f"âœ“ DuckDBå†™å…¥å™¨å·²åœæ­¢ ({self.data_type})")
    
    def _flush_day_sync(self, trading_day: str, dfs: List[pd.DataFrame]) -> None:
        """
        åŒæ­¥åˆ·æ–°ï¼ˆstopæ—¶ä½¿ç”¨ï¼Œé¿å…å¯åŠ¨æ–°çº¿ç¨‹ï¼‰
        
        Args:
            trading_day: äº¤æ˜“æ—¥æœŸ
            dfs: å¾…åˆ·æ–°çš„DataFrameåˆ—è¡¨
        """
        self._flush_day_async(trading_day, dfs)
    
    def get_stats(self) -> Dict:
        """
        è·å–å†™å…¥å™¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¼ºç‰ˆï¼šåŒ…å«çº¿ç¨‹ç›‘æ§ï¼‰
        
        Returns:
            {
                'batch_threshold': int,
                'buffer_sizes': Dict[str, int],  # {trading_day: buffer_size}
                'total_buffered': int,
                'thread_stats': Dict  # çº¿ç¨‹ç»Ÿè®¡ä¿¡æ¯
            }
        """
        with self.buffer_lock:
            buffer_sizes = {
                day: sum(len(df) for df in dfs)
                for day, dfs in self.daily_buffer.items()
            }
        
        # è·å–çº¿ç¨‹ç›‘æ§ä¿¡æ¯
        thread_stats = self._monitor_and_cleanup_threads()
        
        return {
            'batch_threshold': self.batch_threshold,
            'buffer_sizes': buffer_sizes,
            'total_buffered': sum(buffer_sizes.values()),
            'thread_stats': thread_stats  # æ–°å¢
        }


class DuckDBQueryEngine:
    """
    DuckDBæŸ¥è¯¢å¼•æ“ - æ”¯æŒå•æ—¥å’Œè·¨æ—¥æŸ¥è¯¢
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. å•æ—¥æŸ¥è¯¢ï¼šç›´æ¥è¯»å•æ–‡ä»¶ï¼ˆæå¿«ï¼Œ~12msï¼‰
    2. è·¨æ—¥æŸ¥è¯¢ï¼šATTACHå¤šæ–‡ä»¶å¹¶è¡Œï¼ˆ~80msï¼‰
    3. Zone Mapsè‡ªåŠ¨è£å‰ªï¼šæ— éœ€åˆ›å»ºç´¢å¼•
    
    æŸ¥è¯¢ç­–ç•¥ï¼š
    - è‡ªåŠ¨åˆ¤æ–­å•æ—¥/è·¨æ—¥
    - è·¨æ—¥æŸ¥è¯¢ä½¿ç”¨UNION ALL + ATTACH
    - DuckDBè‡ªåŠ¨å¹¶è¡Œæ‰«æ
    """
    
    def __init__(self,
                 db_path: str = "data/duckdb/ticks",
                 data_type: str = "ticks"):
        """
        åˆå§‹åŒ–æŸ¥è¯¢å¼•æ“
        
        Args:
            db_path: DuckDBæ–‡ä»¶æ ¹ç›®å½•
            data_type: æ•°æ®ç±»å‹ï¼ˆ"ticks"æˆ–"klines"ï¼‰
        """
        self.db_path = Path(db_path)
        self.data_type = data_type
        self.logger = get_logger(self.__class__.__name__)
    
    def query_ticks(self,
                    instrument_id: str,
                    start_time: str,
                    end_time: str) -> pd.DataFrame:
        """
        æŸ¥è¯¢Tickæ•°æ®ï¼ˆè‡ªåŠ¨åˆ¤æ–­å•æ—¥/è·¨æ—¥ï¼‰
        
        Args:
            instrument_id: åˆçº¦ä»£ç 
            start_time: å¼€å§‹æ—¶é—´ï¼ˆæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SS æˆ– YYYY-MM-DDï¼‰
            end_time: ç»“æŸæ—¶é—´
        
        Returns:
            DataFrame: æŸ¥è¯¢ç»“æœï¼ˆæŒ‰Timestampæ’åºï¼‰
        """
        # è§£ææ—¶é—´
        try:
            start_dt = pd.to_datetime(start_time)
            end_dt = pd.to_datetime(end_time)
        except Exception as e:
            self.logger.error(f"æ—¶é—´æ ¼å¼é”™è¯¯ï¼š{e}")
            return pd.DataFrame()
        
        # è·å–æ¶‰åŠçš„äº¤æ˜“æ—¥
        trading_days = self._get_trading_days_between(
            start_dt.strftime('%Y%m%d'),
            end_dt.strftime('%Y%m%d')
        )
        
        if not trading_days:
            self.logger.warning(f"æœªæ‰¾åˆ°ç›¸å…³äº¤æ˜“æ—¥ï¼š{start_time} ~ {end_time}")
            return pd.DataFrame()
        
        # åˆ¤æ–­å•æ—¥/è·¨æ—¥
        if len(trading_days) == 1:
            # å•æ—¥æŸ¥è¯¢ï¼ˆæœ€å¿«è·¯å¾„ï¼‰
            return self._query_single_day(
                trading_days[0], instrument_id, start_dt, end_dt
            )
        else:
            # è·¨æ—¥æŸ¥è¯¢ï¼ˆATTACHå¤šåº“ï¼‰
            return self._query_multiple_days(
                trading_days, instrument_id, start_dt, end_dt
            )
    
    def _query_single_day(self,
                         trading_day: str,
                         instrument_id: str,
                         start_dt: datetime,
                         end_dt: datetime) -> pd.DataFrame:
        """
        å•æ—¥æŸ¥è¯¢ï¼ˆæœ€å¿«è·¯å¾„ï¼‰
        
        å®ç°ï¼š
        1. æ‰“å¼€å¯¹åº”çš„.duckdbæ–‡ä»¶ï¼ˆåªè¯»æ¨¡å¼ï¼‰
        2. æ‰§è¡ŒæŸ¥è¯¢ï¼šWHERE InstrumentID = ? AND Timestamp BETWEEN ? AND ?
        3. Zone Mapsè‡ªåŠ¨è£å‰ªï¼ˆè·³è¿‡ä¸ç›¸å…³çš„Row Groupsï¼‰
        4. è¿”å›ç»“æœ
        """
        db_file = self.db_path / f"{trading_day}.duckdb"
        
        if not db_file.exists():
            self.logger.warning(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼š{db_file}")
            return pd.DataFrame()
        
        # æ‰“å¼€è¿æ¥ï¼ˆåªè¯»æ¨¡å¼ï¼‰
        conn = duckdb.connect(str(db_file), read_only=True)
        
        try:
            # æ¥æŸ¥è¯¢åˆçº¦è¡¨ï¼ˆå¤©ç„¶ç‰©ç†éš”ç¦»ï¼Œæé€ŸæŸ¥è¯¢ï¼‰
            if self.data_type == 'ticks':
                table_name = f"tick_{normalize_instrument_id(instrument_id)}"
            else:  # klines
                table_name = f"kline_{normalize_instrument_id(instrument_id)}"
            
            # æŸ¥è¯¢ï¼ˆåªéœ€æ—¶é—´è¿‡æ»¤ï¼Œæ— éœ€InstrumentIDè¿‡æ»¤ï¼‰
            query = f"""
                SELECT * FROM {table_name}
                WHERE Timestamp BETWEEN ? AND ?
                ORDER BY Timestamp
            """
            
            df = conn.execute(query, [start_dt, end_dt]).df()
            
            self.logger.debug(
                f"å•æ—¥æŸ¥è¯¢å®Œæˆï¼š{trading_day}/{instrument_id}ï¼ˆè¡¨: {table_name}ï¼‰ï¼Œ{len(df)}æ¡"
            )
            
            return df
            
        except Exception as e:
            # è¡¨å¯èƒ½ä¸å­˜åœ¨ï¼ˆåˆçº¦å½“å¤©æ²¡æœ‰æ•°æ®ï¼‰
            if "does not exist" in str(e) or "not found" in str(e).lower():
                self.logger.debug(f"åˆçº¦è¡¨ä¸å­˜åœ¨ï¼š{table_name}ï¼ˆåˆçº¦å½“å¤©æ— æ•°æ®ï¼‰")
                return pd.DataFrame()
            else:
                self.logger.error(f"å•æ—¥æŸ¥è¯¢å¤±è´¥ [{trading_day}]ï¼š{e}", exc_info=True)
                return pd.DataFrame()
        
        finally:
            conn.close()
    
    def _query_multiple_days(self,
                            trading_days: List[str],
                            instrument_id: str,
                            start_dt: datetime,
                            end_dt: datetime) -> pd.DataFrame:
        """
        è·¨æ—¥æŸ¥è¯¢ï¼ˆATTACHå¤šåº“ï¼‰
        
        å®ç°ï¼š
        1. åˆ›å»ºå†…å­˜è¿æ¥ï¼šduckdb.connect(':memory:')
        2. ATTACHæ‰€æœ‰ç›¸å…³æ—¥æœŸçš„.duckdbæ–‡ä»¶
        3. UNION ALLæŸ¥è¯¢
        4. DuckDBè‡ªåŠ¨å¹¶è¡Œæ‰«æ
        5. æŒ‰Timestampæ’åºè¿”å›
        """
        # æ„å»ºæ–‡ä»¶åˆ—è¡¨ï¼ˆè¿‡æ»¤ä¸å­˜åœ¨çš„æ–‡ä»¶ï¼‰
        db_files = [
            (str(self.db_path / f"{day}.duckdb"), day)
            for day in trading_days
            if (self.db_path / f"{day}.duckdb").exists()
        ]
        
        if not db_files:
            self.logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•æ•°æ®åº“æ–‡ä»¶ï¼š{trading_days}")
            return pd.DataFrame()
        
        # åˆ›å»ºå†…å­˜è¿æ¥
        conn = duckdb.connect(':memory:')
        
        try:
            # ATTACHæ‰€æœ‰ç›¸å…³æ—¥æœŸçš„æ•°æ®åº“
            for i, (db_file, day) in enumerate(db_files):
                conn.execute(f"ATTACH '{db_file}' AS db{i} (READ_ONLY)")
                self.logger.debug(f"ATTACHæ•°æ®åº“ï¼šdb{i} <- {day}")
            
            # æ„å»ºUNION ALLæŸ¥è¯¢ï¼ˆæŸ¥è¯¢å„æ–‡ä»¶çš„åˆçº¦è¡¨ï¼‰
            if self.data_type == 'ticks':
                table_name = f"tick_{normalize_instrument_id(instrument_id)}"
            else:  # klines
                table_name = f"kline_{normalize_instrument_id(instrument_id)}"
            
            union_queries = [
                f"""
                SELECT * FROM db{i}.{table_name}
                WHERE Timestamp BETWEEN '{start_dt}' AND '{end_dt}'
                """
                for i in range(len(db_files))
            ]
            
            query = " UNION ALL ".join(union_queries) + " ORDER BY Timestamp"
            
            # æ‰§è¡ŒæŸ¥è¯¢ï¼ˆDuckDBè‡ªåŠ¨å¹¶è¡Œï¼‰
            df = conn.execute(query).df()
            
            self.logger.info(
                f"è·¨æ—¥æŸ¥è¯¢å®Œæˆï¼š{len(db_files)}ä¸ªæ–‡ä»¶ï¼Œ{instrument_id}ï¼ˆè¡¨: {table_name}ï¼‰ï¼Œ"
                f"ç»“æœ={len(df)}æ¡"
            )
            
            return df
            
        except Exception as e:
            self.logger.error(
                f"è·¨æ—¥æŸ¥è¯¢å¤±è´¥ [{trading_days}]ï¼š{e}",
                exc_info=True
            )
            return pd.DataFrame()
        
        finally:
            conn.close()
    
    def _get_trading_days_between(self,
                                  start_date: str,
                                  end_date: str) -> List[str]:
        """
        è·å–ä¸¤ä¸ªæ—¥æœŸä¹‹é—´çš„æ‰€æœ‰äº¤æ˜“æ—¥
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰
            end_date: ç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYYMMDDï¼‰
        
        Returns:
            ['20251027', '20251028', '20251029', ...]
        
        Note:
            ç®€åŒ–å®ç°ï¼šè¿”å›æ‰€æœ‰æ—¥æœŸï¼ˆåŒ…æ‹¬éäº¤æ˜“æ—¥ï¼‰
            å®é™…ç”Ÿäº§ç¯å¢ƒåº”è¯¥ä»äº¤æ˜“æ—¥å†è·å–
        """
        try:
            start_dt = datetime.strptime(start_date, '%Y%m%d')
            end_dt = datetime.strptime(end_date, '%Y%m%d')
        except ValueError:
            self.logger.error(f"æ—¥æœŸæ ¼å¼é”™è¯¯ï¼š{start_date}, {end_date}")
            return []
        
        # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨
        trading_days = []
        current_dt = start_dt
        
        while current_dt <= end_dt:
            # åªæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
            day_str = current_dt.strftime('%Y%m%d')
            db_file = self.db_path / f"{day_str}.duckdb"
            
            if db_file.exists():
                trading_days.append(day_str)
            
            current_dt += timedelta(days=1)
        
        return trading_days

