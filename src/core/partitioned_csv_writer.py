#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@ProjectName: homalos-datacenter
@FileName   : partitioned_csv_writer.py
@Date       : 2025/10/29
@Author     : Lumosylva
@Email      : donnymoving@gmail.com
@Software   : PyCharm
@Description: å¤šçº¿ç¨‹+å“ˆå¸Œåˆ†é…CSVå†™å…¥å™¨
"""
import threading
import queue
import hashlib
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional
from collections import defaultdict
from datetime import datetime

from src.utils.log import get_logger
from src.core.trading_day_manager import TradingDayManager


class PartitionedCSVWriter:
    """
    å¤šçº¿ç¨‹+å“ˆå¸Œåˆ†é…CSVå†™å…¥å™¨
    
    æ ¸å¿ƒæ¶æ„ï¼š
    1. å›ºå®šçº¿ç¨‹æ± ï¼ˆé»˜è®¤4çº¿ç¨‹ï¼‰
    2. åˆçº¦å“ˆå¸Œåˆ†é…ï¼šhash(InstrumentID) % thread_count
    3. æ¯çº¿ç¨‹ç‹¬ç«‹é˜Ÿåˆ—+ç¼“å†²åŒº
    4. æ‰¹é‡å†™å…¥ï¼ˆé˜ˆå€¼è§¦å‘ï¼‰
    
    æ€§èƒ½ä¼˜åŠ¿ï¼š
    - è´Ÿè½½å‡è¡¡ï¼š820åˆçº¦å‡åˆ†4çº¿ç¨‹ï¼ˆæ¯çº¿ç¨‹~205åˆçº¦ï¼‰
    - å¹¶è¡Œå†™å…¥ï¼šä¸åŒåˆçº¦å¯å¹¶å‘å†™å…¥
    - æ‰¹é‡ä¼˜åŒ–ï¼šå‡å°‘æ–‡ä»¶IOæ¬¡æ•°
    - æ–‡ä»¶é”ä¿æŠ¤ï¼šé˜²æ­¢å¹¶å‘å†²çª
    
    ååæå‡ï¼š
    - 820åˆçº¦å¹¶å‘å†™å…¥ï¼š~3.5x
    - å•åˆçº¦å¯†é›†å†™å…¥ï¼š~1.2x
    """
    
    def __init__(self,
                 base_path: str = "data/csv/ticks",
                 num_threads: int = 4,
                 batch_threshold: int = 5000,
                 queue_max_size: int = 50000,
                 trading_day_manager: Optional[TradingDayManager] = None):
        """
        åˆå§‹åŒ–åˆ†åŒºå†™å…¥å™¨
        
        Args:
            base_path: CSVæ–‡ä»¶æ ¹ç›®å½•
            num_threads: å·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤4ï¼‰
            batch_threshold: æ‰¹é‡å†™å…¥é˜ˆå€¼ï¼ˆæ¯çº¿ç¨‹ç´¯ç§¯å¤šå°‘æ¡è§¦å‘å†™å…¥ï¼‰
            queue_max_size: æ¯ä¸ªé˜Ÿåˆ—æœ€å¤§å¤§å°ï¼ˆé˜²æ­¢å†…å­˜æº¢å‡ºï¼‰
            trading_day_manager: äº¤æ˜“æ—¥ç®¡ç†å™¨
        """
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
        self.num_threads = num_threads
        self.batch_threshold = batch_threshold
        self.trading_day_manager = trading_day_manager
        self.logger = get_logger(self.__class__.__name__)
        
        # æ¯ä¸ªçº¿ç¨‹çš„é˜Ÿåˆ—
        self.queues: List[queue.Queue] = [
            queue.Queue(maxsize=queue_max_size)
            for _ in range(num_threads)
        ]
        
        # æ–‡ä»¶é”å­—å…¸ï¼ˆä¸DataStorageå…±äº«é”æœºåˆ¶ï¼‰
        self._file_locks: Dict[str, threading.Lock] = {}
        self._locks_lock = threading.Lock()
        
        # å·¥ä½œçº¿ç¨‹
        self.workers: List[threading.Thread] = []
        self._stop_event = threading.Event()
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        self._start_workers()
    
    def _hash_instrument(self, instrument_id: str) -> int:
        """
        è®¡ç®—åˆçº¦ä»£ç çš„å“ˆå¸Œå€¼ï¼Œåˆ†é…åˆ°çº¿ç¨‹
        
        Args:
            instrument_id: åˆçº¦ä»£ç 
        
        Returns:
            çº¿ç¨‹ç´¢å¼•ï¼ˆ0 ~ num_threads-1ï¼‰
        
        å®ç°ï¼š
            ä½¿ç”¨MD5å“ˆå¸Œä¿è¯åˆ†å¸ƒå‡åŒ€ï¼ˆæ¯”Pythonå†…ç½®hashæ›´å‡åŒ€ï¼‰
        """
        # ä½¿ç”¨MD5å“ˆå¸Œä¿è¯åˆ†å¸ƒå‡åŒ€
        hash_value = int(hashlib.md5(instrument_id.encode()).hexdigest()[:8], 16)
        return hash_value % self.num_threads
    
    def _get_file_lock(self, file_path: Path) -> threading.Lock:
        """
        è·å–æ–‡ä»¶é”ï¼ˆä¸DataStorageæœºåˆ¶ä¸€è‡´ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            æ–‡ä»¶é”å¯¹è±¡
        """
        file_key = str(file_path.resolve())
        
        with self._locks_lock:
            if file_key not in self._file_locks:
                self._file_locks[file_key] = threading.Lock()
            return self._file_locks[file_key]
    
    def _start_workers(self) -> None:
        """å¯åŠ¨æ‰€æœ‰å·¥ä½œçº¿ç¨‹"""
        for i in range(self.num_threads):
            worker = threading.Thread(
                target=self._worker_loop,
                args=(i,),
                name=f"CSVWriter-Worker-{i}",
                daemon=True
            )
            worker.start()
            self.workers.append(worker)
        
        self.logger.info(
            f"CSVå†™å…¥å™¨å·²å¯åŠ¨ï¼š{self.num_threads}ä¸ªå·¥ä½œçº¿ç¨‹ï¼Œ"
            f"æ‰¹é‡é˜ˆå€¼ï¼š{self.batch_threshold}æ¡"
        )
    
    def _worker_loop(self, thread_id: int) -> None:
        """
        å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯
        
        Args:
            thread_id: çº¿ç¨‹IDï¼ˆ0 ~ num_threads-1ï¼‰
        
        å®ç°æµç¨‹ï¼š
        1. ä»é˜Ÿåˆ—è·å–æ•°æ®ï¼ˆé˜»å¡ï¼Œè¶…æ—¶1ç§’ï¼‰
        2. æ·»åŠ åˆ°ç¼“å†²åŒº
        3. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹é‡é˜ˆå€¼
        4. è¾¾åˆ°é˜ˆå€¼ â†’ è°ƒç”¨_flush_buffer
        5. å¾ªç¯ç›´åˆ°æ”¶åˆ°åœæ­¢ä¿¡å·
        6. é€€å‡ºå‰åˆ·æ–°å‰©ä½™æ•°æ®
        """
        q = self.queues[thread_id]
        # ç¼“å†²åŒºï¼š{InstrumentID: [DataFrame1, DataFrame2, ...]}
        buffer: Dict[str, List[pd.DataFrame]] = defaultdict(list)
        buffer_size = 0  # å½“å‰ç¼“å†²åŒºæ€»è¡Œæ•°
        current_trading_day = None  # å½“å‰äº¤æ˜“æ—¥
        
        self.logger.info(f"Worker-{thread_id} å·²å¯åŠ¨")
        
        while not self._stop_event.is_set():
            try:
                # é˜»å¡è·å–æ•°æ®ï¼ˆè¶…æ—¶1ç§’ï¼Œé¿å…æ— æ³•é€€å‡ºï¼‰
                try:
                    item = q.get(timeout=1.0)
                except queue.Empty:
                    # è¶…æ—¶æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
                    if self._stop_event.is_set():
                        break
                    continue
                
                if item is None:  # å“¨å…µå€¼ï¼Œè¡¨ç¤ºåœæ­¢
                    break
                
                instrument_id, df, trading_day = item
                
                # å¦‚æœäº¤æ˜“æ—¥å˜åŒ–ï¼Œåˆ·æ–°ä¹‹å‰çš„æ•°æ®
                if current_trading_day and trading_day != current_trading_day:
                    if buffer:
                        self._flush_buffer(thread_id, buffer, current_trading_day)
                        buffer.clear()
                        buffer_size = 0
                
                current_trading_day = trading_day
                
                # æ·»åŠ åˆ°ç¼“å†²åŒº
                buffer[instrument_id].append(df)
                buffer_size += len(df)
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹é‡é˜ˆå€¼
                if buffer_size >= self.batch_threshold:
                    self._flush_buffer(thread_id, buffer, trading_day)
                    buffer.clear()
                    buffer_size = 0
                
                q.task_done()
                
            except Exception as e:
                self.logger.error(
                    f"Worker-{thread_id} å¤„ç†æ•°æ®æ—¶å‡ºé”™ï¼š{e}",
                    exc_info=True
                )
        
        # çº¿ç¨‹é€€å‡ºå‰åˆ·æ–°å‰©ä½™æ•°æ®
        if buffer and current_trading_day:
            self.logger.info(
                f"Worker-{thread_id} é€€å‡ºå‰åˆ·æ–°å‰©ä½™ {buffer_size} æ¡æ•°æ®"
            )
            self._flush_buffer(thread_id, buffer, current_trading_day)
        
        self.logger.info(f"Worker-{thread_id} å·²åœæ­¢")
    
    def _flush_buffer(self,
                     thread_id: int,
                     buffer: Dict[str, List[pd.DataFrame]],
                     trading_day: str) -> None:
        """
        åˆ·æ–°ç¼“å†²åŒºåˆ°CSVæ–‡ä»¶
        
        Args:
            thread_id: çº¿ç¨‹ID
            buffer: {InstrumentID: [df1, df2, ...]}
            trading_day: äº¤æ˜“æ—¥æœŸ
        
        å®ç°ï¼š
        1. éå†bufferä¸­çš„æ¯ä¸ªåˆçº¦
        2. åˆå¹¶è¯¥åˆçº¦çš„æ‰€æœ‰DataFrame
        3. è·å–æ–‡ä»¶è·¯å¾„ï¼šbase_path/trading_day/instrument_id.csv
        4. è·å–æ–‡ä»¶é”
        5. è¿½åŠ å†™å…¥CSVï¼ˆæ£€æŸ¥æ˜¯å¦éœ€è¦å†™è¡¨å¤´ï¼‰
        """
        total_rows = sum(sum(len(df) for df in dfs) for dfs in buffer.values())
        
        for instrument_id, dfs in buffer.items():
            # åˆå¹¶æ‰€æœ‰DataFrame
            merged_df = pd.concat(dfs, ignore_index=True)
            
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            date_dir = self.base_path / trading_day
            date_dir.mkdir(parents=True, exist_ok=True)
            file_path = date_dir / f"{instrument_id}.csv"
            
            # è·å–æ–‡ä»¶é”
            file_lock = self._get_file_lock(file_path)
            
            with file_lock:
                try:
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    file_exists = file_path.exists() and file_path.stat().st_size > 0
                    
                    # è¿½åŠ å†™å…¥
                    merged_df.to_csv(
                        file_path,
                        mode='a',
                        header=not file_exists,
                        index=False
                    )
                    
                except Exception as e:
                    self.logger.error(
                        f"Worker-{thread_id} å†™å…¥CSVå¤±è´¥ [{instrument_id}]ï¼š{e}",
                        exc_info=True
                    )
        
        self.logger.debug(
            f"Worker-{thread_id} æ‰¹é‡å†™å…¥å®Œæˆï¼š{total_rows}æ¡ï¼Œ"
            f"{len(buffer)}ä¸ªåˆçº¦"
        )
    
    def submit_batch(self, df: pd.DataFrame, trading_day: Optional[str] = None) -> None:
        """
        æäº¤ä¸€æ‰¹æ•°æ®ï¼ˆæŒ‰åˆçº¦å“ˆå¸Œåˆ†é…åˆ°çº¿ç¨‹ï¼‰
        
        Args:
            df: æ•°æ®DataFrameï¼ˆå¿…é¡»åŒ…å«InstrumentIDåˆ—ï¼‰
            trading_day: äº¤æ˜“æ—¥æœŸï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨TradingDayManager
        
        å®ç°ï¼š
        1. éªŒè¯DataFrame
        2. æŒ‰InstrumentIDåˆ†ç»„
        3. å¯¹æ¯ä¸ªåˆçº¦ï¼š
           a. è®¡ç®—çº¿ç¨‹ç´¢å¼•ï¼š_hash_instrument(instrument_id)
           b. æäº¤åˆ°å¯¹åº”é˜Ÿåˆ—ï¼šqueues[thread_idx].put((instrument_id, group_df, trading_day))
        """
        if df.empty or "InstrumentID" not in df.columns:
            self.logger.warning("æäº¤çš„DataFrameä¸ºç©ºæˆ–ç¼ºå°‘InstrumentIDåˆ—")
            return
        
        # è·å–äº¤æ˜“æ—¥
        if trading_day is None:
            if self.trading_day_manager:
                trading_day = self.trading_day_manager.get_trading_day()
            else:
                trading_day = datetime.now().strftime("%Y%m%d")
        
        # æŒ‰åˆçº¦åˆ†ç»„
        for instrument_id, group_df in df.groupby("InstrumentID"):
            # è®¡ç®—çº¿ç¨‹ç´¢å¼•
            thread_idx = self._hash_instrument(instrument_id)
            
            # ğŸ”¥ æ”¹è¿›ï¼šé˜Ÿåˆ—æ»¡æ—¶é™çº§å¤„ç†
            try:
                self.queues[thread_idx].put(
                    (instrument_id, group_df, trading_day),
                    timeout=5.0  # 5ç§’è¶…æ—¶
                )
            except queue.Full:
                # ğŸ”¥ é™çº§ç­–ç•¥ï¼šç›´æ¥å†™æ–‡ä»¶ï¼ˆç»•è¿‡é˜Ÿåˆ—ï¼Œä¿è¯æ•°æ®ä¸ä¸¢å¤±ï¼‰
                self.logger.error(
                    f"é˜Ÿåˆ—{thread_idx}å·²æ»¡ï¼Œé™çº§ä¸ºç›´æ¥å†™å…¥ï¼š{instrument_id}ï¼Œ{len(group_df)}æ¡"
                )
                self._write_directly(instrument_id, group_df, trading_day)
    
    def stop(self, timeout: float = 30.0) -> None:
        """
        åœæ­¢å†™å…¥å™¨ï¼Œåˆ·æ–°æ‰€æœ‰å‰©ä½™æ•°æ®
        
        Args:
            timeout: ç­‰å¾…çº¿ç¨‹é€€å‡ºçš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
        å®ç°ï¼š
        1. è®¾ç½®åœæ­¢æ ‡å¿—ï¼š_stop_event.set()
        2. å‘æ‰€æœ‰é˜Ÿåˆ—å‘é€å“¨å…µå€¼ï¼šNone
        3. ç­‰å¾…æ‰€æœ‰çº¿ç¨‹é€€å‡ºï¼šworker.join(timeout)
        """
        self.logger.info("æ­£åœ¨åœæ­¢CSVå†™å…¥å™¨...")
        
        # è®¾ç½®åœæ­¢æ ‡å¿—
        self._stop_event.set()
        
        # å‘æ‰€æœ‰é˜Ÿåˆ—å‘é€å“¨å…µå€¼
        for q in self.queues:
            try:
                q.put(None, timeout=1.0)
            except queue.Full:
                self.logger.warning("é˜Ÿåˆ—å·²æ»¡ï¼Œæ— æ³•å‘é€åœæ­¢ä¿¡å·")
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹é€€å‡º
        for i, worker in enumerate(self.workers):
            worker.join(timeout=timeout)
            if worker.is_alive():
                self.logger.warning(f"Worker-{i} æœªèƒ½åœ¨{timeout}ç§’å†…åœæ­¢")
            else:
                self.logger.info(f"Worker-{i} å·²åœæ­¢")
        
        self.logger.info("CSVå†™å…¥å™¨å·²åœæ­¢")
    
    def get_stats(self) -> Dict:
        """
        è·å–å†™å…¥å™¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            {
                'num_threads': int,
                'batch_threshold': int,
                'queue_sizes': List[int],
                'total_queued': int,
                'workers_alive': int
            }
        """
        return {
            'num_threads': self.num_threads,
            'batch_threshold': self.batch_threshold,
            'queue_sizes': [q.qsize() for q in self.queues],
            'total_queued': sum(q.qsize() for q in self.queues),
            'workers_alive': sum(1 for w in self.workers if w.is_alive())
        }
    
    def _write_directly(self, instrument_id: str, df: pd.DataFrame, trading_day: str) -> None:
        """
        ç›´æ¥å†™å…¥CSVæ–‡ä»¶ï¼ˆç»•è¿‡é˜Ÿåˆ—çš„é™çº§ç­–ç•¥ï¼‰
        
        Args:
            instrument_id: åˆçº¦ä»£ç 
            df: æ•°æ®DataFrame
            trading_day: äº¤æ˜“æ—¥æœŸ
        
        ğŸ”¥ ç”¨é€”ï¼šå½“é˜Ÿåˆ—æ»¡æ—¶ï¼Œç›´æ¥å†™æ–‡ä»¶ä¿è¯æ•°æ®ä¸ä¸¢å¤±
        """
        try:
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            date_dir = self.base_path / trading_day
            date_dir.mkdir(parents=True, exist_ok=True)
            file_path = date_dir / f"{instrument_id}.csv"
            
            # è·å–æ–‡ä»¶é”
            file_lock = self._get_file_lock(file_path)
            
            with file_lock:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                file_exists = file_path.exists() and file_path.stat().st_size > 0
                
                # è¿½åŠ å†™å…¥
                df.to_csv(
                    file_path,
                    mode='a',
                    header=not file_exists,
                    index=False
                )
                
                self.logger.info(
                    f"âœ“ é™çº§ç›´æ¥å†™å…¥æˆåŠŸï¼š{instrument_id}ï¼Œ{len(df)}æ¡ â†’ {file_path}"
                )
        
        except Exception as e:
            self.logger.error(
                f"é™çº§ç›´æ¥å†™å…¥å¤±è´¥ [{instrument_id}]ï¼š{e}",
                exc_info=True
            )
            # ğŸš¨ ä¸¥é‡é”™è¯¯ï¼šè®°å½•åˆ°å•ç‹¬çš„å¤±è´¥æ—¥å¿—
            self._log_critical_failure(instrument_id, len(df), trading_day, str(e))
    
    def _log_critical_failure(self, instrument_id: str, row_count: int, 
                              trading_day: str, error: str) -> None:
        """
        è®°å½•ä¸¥é‡çš„å†™å…¥å¤±è´¥ï¼ˆæ•°æ®å¯èƒ½ä¸¢å¤±ï¼‰
        
        Args:
            instrument_id: åˆçº¦ä»£ç 
            row_count: ä¸¢å¤±çš„æ•°æ®è¡Œæ•°
            trading_day: äº¤æ˜“æ—¥æœŸ
            error: é”™è¯¯ä¿¡æ¯
        """
        try:
            failed_log = self.base_path / "failed_writes.log"
            with open(failed_log, "a", encoding="utf-8") as f:
                f.write(
                    f"{datetime.now().isoformat()} | {trading_day} | "
                    f"{instrument_id} | {row_count}æ¡ | é”™è¯¯: {error}\n"
                )
            self.logger.critical(
                f"ğŸš¨ ä¸¥é‡ï¼šæ•°æ®å¯èƒ½ä¸¢å¤±ï¼å·²è®°å½•åˆ° {failed_log}"
            )
        except Exception as e:
            self.logger.error(f"è®°å½•å¤±è´¥æ—¥å¿—æ—¶å‡ºé”™ï¼š{e}")

